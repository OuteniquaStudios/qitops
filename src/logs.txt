 +1,6 @@\n from dataclasses import dataclass\n-from typing import List\n+from typing import List, Optional\n+from datetime import datetime\n \n @dataclass\n class TestCase:\n@@ -8,4 +9,8 @@ class TestCase:\n     priority: str\n     description: str\n     steps: List[str]\n-    expected_result: str\n\\ No newline at end of file\n+    expected_result: str\n+    generated_at: datetime = datetime.now()\n+    approved: bool = False\n+    approved_by: Optional[str] = None\n+    risk_factors: List[str] = None\n\\ No newline at end of file\n```\n\nFile: src/pr_test_cases.yaml\n```diff\n@@ -1,53 +1,71 @@\n pr_number: 1\n pr_title: Bump starlette from 0.27.0 to 0.40.0 in /src\n risk_analysis:\n-  level: Medium\n+  level: High\n   factors:\n-  - []\n+  - Security Risk\n+  - Dependency Changes\n+  details:\n+  - Security-sensitive code changes detected\n+  - Package dependencies modified\n test_cases:\n - id: TC-001\n-  title: No title\n-  priority: Medium\n-  description: No description\n+  title: Verify the security of the updated Starlette dependency in the application\n+  priority: High\n+  description: Check if any newly introduced vulnerabilities or security risks are\n+    present in the updated version (0.40.0) of Starlette, which could potentially\n+    impact the overall security of the application.\n   steps:\n-  - Import starlette module in a test script\n-  - Write a test function which was working with the previous version and check if\n-    it works as expected with the new version\n+  - Install the updated version of Starlette (0.40.0) in a separate development environment\n+  - Run a comprehensive vulnerability scan on the isolated environment using tools\n+    like OWASP ZAP or Bandit\n+  - Check for any reported vulnerabilities, security issues, or warnings related to\n+    the updated Starlette package\n   - \'\'\n-  expected_result: The test function should pass without any errors or unexpected\n-    behaviors.\n+  expected_result: The security scan should not reveal any critical or high-severity\n+    vulnerabilities, ensuring that the application remains secure. If any issues are\n+    found, they must be addressed and fixed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.979267\'\n+  approved: false\n+  
approved_by: null\n - id: TC-002\n-  title: No title\n+  title: Ensure compatibility of the updated Starlette dependency with other dependencies\n+    in the application\n   priority: Medium\n-  description: No description\n+  description: Validate that the updated version (0.40.0) of Starlette is compatible\n+    with the existing versions of other dependencies within the application, to prevent\n+    any potential conflicts or errors during runtime.\n   steps:\n-  - Install and run OWASP ZAP or bandit on the new starlette codebase\n-  - Analyze the results for any potential security vulnerabilities\n+  - Create a development environment containing all the dependencies listed in the\n+    requirements.txt file (excluding Starlette for now)\n+  - Install the updated version of Starlette (0.40.0) in this environment\n+  - Run the application to check if there are 
any errors, warnings, or unexpected\n+    behavior caused by the updated Starlette dependency\n   - \'\'\n-  expected_result: The test should not find any critical or high severity security\n-    vulnerabilities in the new version.\n+  expected_result: The application should run smoothly without any critical errors\n+    or warning messages related to the updated Starlette package, demonstrating compatibility\n+    with other dependencies. If any issues occur, they must be resolved before merging\n+    this update.\n+  
generated_at: \'2025-01-28T14:27:14.979975\'\n+  approved: false\n+  approved_by: null\n - id: TC-003\n-  title: No title\n-  priority: Medium\n-  description: No description\n-  steps:\n-  - Create a custom exception class\n-  - Use the custom exception in a route handler and trigger it intentionally\n-  - Check if the exception is handled correctly by starlette and an appropriate error\n-    message is returned to the client\n-  - \'\'\n-  expected_result: Starlette should be able to handle the custom exception gracefully,\n-    returning an appropriate error message to the client.\n-- id: TC-004\n-  title: No title\n+  title: Test error handling of the updated Starlette dependency in the 
application\n   priority: Medium\n-  description: No description\n+  description: Verify that the updated version (0.40.0) of Starlette handles errors\n+    and exceptions gracefully, ensuring a good user experience even when unexpected\n+    situations occur.\n   steps:\n-  - Create a test script which uses an API or functionality from starlette 0.27.0\n-  - Run the test script with both versions (0.27.0 and 0.40.0) of starlette installed\n-    separately\n-  - Check if there are any differences in behavior or outcomes between the two versions\n+  - Create test scenarios where exceptional conditions are intentionally triggered\n+    within the application (e.g., by providing invalid inputs 
or forcing certain edge\n+    cases)\n+  - Observe how the updated Starlette dependency handles these exceptions and errors,\n+    checking for appropriate logging, error messages, and recovery mechanisms\n   - \'\'\n-  expected_result: There should not be any noticeable differences in behavior or outcomes\n-    when using the same API or functionality with both versions (0.27.0 and 0.40.0)\n-    of starlette.\n+  expected_result: The updated Starlette dependency should handle errors effectively,\n+    providing descriptive error messages to help troubleshoot issues and maintaining\n+    application stability in the event of exceptional conditions. If any issues occur,\n+    they must be addressed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.980992\'\n+  approved: false\n+  approved_by: null\n```\n\nFile: src/prompts/pr_test_case_prompt.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/prompts/templates/test_case.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/services/llm/llm_service.py\n```diff\n@@ -27,29 +27,21 @@ def generate(self, prompt: str, context: Dict[str, Any]) -> str:\n         return result\n \n     def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:\n-        pr = context.get(\'pr\', {})\n-        risk = context.get(\'risk_analysis\', {})\n-        \n-        # Format changes\n-        changes_str = "\\n".join(\n-            f"{k}: {\', \'.join(v)}" \n-            for k, v in pr.get(\'changes\', {}).items() \n-            if v\n-        )\n-        \n-        # Format diffs\n-       
 diffs_str = "\\n".join(\n-            f"File: {fname}\\n{diff}" \n-            for fname, diff in pr.get(\'diffs\', {}).items()\n-        )\n-        \n-        return prompt.format(\n-            pr_title=pr.get(\'title\', \'\'),\n-            pr_description=pr.get(\'description\', \'\'),\n-            risk_level=risk.get(\'level\', \'\'),\n-    
        changes=changes_str,\n-            diffs=diffs_str\n-        )\n+        try:\n+            return prompt.format(\n+                pr_title=context.get(\'pr_title\', 
\'\'),\n+                pr_description=context.get(\'pr_description\', \'\'),\n+                risk_level=context.get(\'risk_level\', \'\'),\n+                risk_factors=context.get(\'risk_factors\', \'\'),\n+                changes=context.get(\'changes\', \'\'),\n+                diffs=context.get(\'diffs\', \'\')\n+            )\n+        except KeyError as e:\n+            self.logger.error(f"Missing required key in context: {e}")\n+            raise\n+        except Exception as e:\n+            self.logger.error(f"Error formatting prompt: {e}")\n+            raise\n \n     def _format_changes(self, changes: Dict[str, Any]) -> str:\n         result = []\n```\n\nFile: src/utils/file_utils.py\n```diff\n@@ -1,5 +1,40 @@\n+import os\n import yaml\n+import logging\n+from typing import Dict, Any\n+import re\n \n-def load_config(path: str) -> dict:\n-    with open(path, \'r\', encoding=\'utf-8\') as f:\n-        return yaml.safe_load(f)\n\\ No newline at end of file\n+def load_config(path: str) -> Dict[str, Any]:\n+    logger = logging.getLogger(__name__)\n+    \n+    if not os.path.exists(path):\n+        logger.error(f"Config file not found: {path}")\n+        raise FileNotFoundError(f"Config file not 
found: {path}")\n+        \n+    with open(path, \'r\') as f:\n+        config = yaml.safe_load(f)\n+        \n+    if not config:\n+        raise ValueError("Empty config file")\n+        \n+    # Replace environment variables\n+    config = _replace_env_vars(config)\n+    \n+    return config\n+\n+def _replace_env_vars(config: Dict[str, Any]) -> 
Dict[str, Any]:\n+    """Recursively replace ${VAR} with environment variable values"""\n+    if isinstance(config, dict):\n+        return {k: _replace_env_vars(v) for k, v in config.items()}\n+    elif isinstance(config, list):\n+        return [_replace_env_vars(v) for v in config]\n+    elif isinstance(config, str):\n+        pattern = r\'\\${([^}]+)}\'\n+        match = re.search(pattern, config)\n+        if match:\n+            env_var = match.group(1)\n+            env_value = os.getenv(env_var)\n+            if not env_value:\n+                raise ValueError(f"Environment variable {env_var} not set")\n+            return config.replace(match.group(0), env_value)\n+    return config\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analysis.py\n```diff\n@@ -1,24 +1,45 @@\n from enum import Enum\n+from typing import Dict, List, Any\n+import re\n \n class RiskLevel(Enum):\n     LOW = "Low"\n     MEDIUM = "Medium"\n     HIGH = "High"\n \n-def analyze_diff(diff_content):\n-    """Analyze diff content for risk patterns."""\n-    risk_patterns = {\n-        "sql": "Database changes",\n-        "await": "Async flow changes",\n-        "try": "Error handling changes",\n-        "import": "Dependency changes",\n-        "class": "Class structure changes",\n-        "function": "Function signature changes"\n-    }\n+class RiskFactor(Enum):\n+    DEPENDENCY = "Dependency Changes"\n+    SECURITY = "Security Impact"\n+    BREAKING = "Breaking Changes"\n+    COVERAGE = "Test Coverage"\n \n-    found_patterns = []\n-    for pattern, risk in risk_patterns.items():\n-        if pattern in diff_content.lower():\n-            found_patterns.append(risk)\n-\n-    return found_patterns\n\\ No newline at end of file\n+def analyze_risk(changes: Dict[str, List[str]], diffs: Dict[str, str]) -> Dict[str, Any]:\n+    risk_factors = []\n+    \n+    # Check dependency changes\n+    if any(\'requirements.txt\' in file or \'package.json\' in file for file in changes.get(\'modified\', [])):\n+        risk_factors.append(RiskFactor.DEPENDENCY.value)\n+    \n+    # Check security patterns\n+    security_patterns = [\n+        r\'auth[orization]*\',\n+        r\'password\',\n+        r\'secret\',\n+        r\'token\',\n+        r\'crypt\'\n+    ]\n+  
  \n+    if any(re.search(\'|\'.join(security_patterns), diff, re.I) for diff in diffs.values()):\n+        risk_factors.append(RiskFactor.SECURITY.value)\n+    \n+    # Determine risk level\n+    risk_level = RiskLevel.LOW\n+    if len(risk_factors) >= 2:\n+        risk_level = RiskLevel.HIGH\n+    elif len(risk_factors) == 1:\n+        risk_level = RiskLevel.MEDIUM\n+    \n+    return {\n+        "level": risk_level.value,\n+        "factors": risk_factors\n+    }\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analyzer.py\n```diff\n@@ -0,0 +1,85 @@\n+from typing import Dict, List, Any, Union\n+from enum import Enum\n+import re\n+import logging\n+\n+class RiskLevel(Enum):\n+    
LOW = "Low"\n+    MEDIUM = "Medium"\n+    HIGH = "High"\n+\n+class RiskAnalyzer:\n+    def __init__(self):\n+        self.logger = logging.getLogger(__name__)\n+        self.security_patterns = [\n+            r\'auth\\w*\',\n+            r\'password\',\n+            r\'secret\',\n+            r\'token\',\n+            r\'crypt\'\n+        ]\n+    
    self.breaking_patterns = [\n+            r\'break.*change\',\n+            r\'deprecat\\w*\',\n+            r\'remov\\w+\\s+\\w+\',\n+            r\'delet\\w+\\s+\\w+\'\n+        ]\n+\n+    def analyze(self, changes: Union[Dict[str, List[str]], str], diffs: Union[Dict[str, str], str]) -> Dict[str, Any]:\n+        try:\n+            # Normalize 
inputs\n+            changes_dict = changes if isinstance(changes, dict) else {"modified": [str(changes)]}\n+            diffs_dict = diffs if isinstance(diffs, dict) else {"file": str(diffs)}\n+            \n+            risk_factors = []\n+            details = []\n+\n+            # Check security risks\n+            if self._check_security_risks(diffs_dict):\n+                risk_factors.append("Security Risk")\n+                details.append("Security-sensitive code changes detected")\n+\n+            # Check dependency changes\n+            if self._check_dependency_changes(changes_dict):\n+                risk_factors.append("Dependency Changes")\n+                details.append("Package dependencies modified")\n+\n+            # Check breaking changes\n+            if self._check_breaking_changes(diffs_dict):\n+                risk_factors.append("Breaking Changes")\n+                details.append("Breaking changes detected")\n+\n+            level = self._determine_risk_level(risk_factors)\n+            \n+            return {\n+                "level": level,\n+                "factors": risk_factors,\n+                "details": details\n+            }\n+        except Exception as e:\n+      
      self.logger.error(f"Error in risk analysis: {e}")\n+            return {\n+                "level": "High",\n+                "factors": ["Analysis Error"],\n+
      "details": [str(e)]\n+            }\n+\n+    def _check_security_risks(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.security_patterns)\n+            for content in diffs.values()\n+        )\n+\n+    def _check_dependency_changes(self, changes: Dict[str, List[str]]) -> bool:\n+        dependency_files = [\'requirements.txt\', \'package.json\', \'build.gradle\', \'pom.xml\']\n+        modified = changes.get(\'modified\', [])\n+  
      return any(dep in str(file) for dep in dependency_files for file in modified)\n+\n+    def _check_breaking_changes(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.breaking_patterns)\n+            for content in diffs.values()\n+        )\n+\n+    def _determine_risk_level(self, factors: List[str]) -> str:\n+        return "High" if len(factors) >= 2 else "Medium" if factors else "Low"\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_patterns.py\n```diff\n@@ -0,0 +1,22 @@\n+from enum import Enum\n+from typing import List, Pattern\n+import re\n+\n+class RiskPatternType(Enum):\n+    SECURITY = "security"\n+    BREAKING = "breaking"\n+    PERFORMANCE = "performance"\n+    COMPLEXITY = "complexity"\n+\n+class RiskPattern:\n+    def __init__(self, pattern: str, risk_type: RiskPatternType, weight: int = 1):\n+        self.pattern = re.compile(pattern, re.IGNORECASE)\n+        self.type = risk_type\n+        self.weight = weight\n+\n+RISK_PATTERNS = [\n+    RiskPattern(r\'auth|login|password|secret|token\', RiskPatternType.SECURITY, 3),\n+    RiskPattern(r\'break.*change|deprecat|remove[d]?\\s+\\w+\', RiskPatternType.BREAKING, 2),\n+    RiskPattern(r\'performance|optimize|slow|fast\', RiskPatternType.PERFORMANCE, 2),\n+    RiskPattern(r\'complex|complicated|confusing\', RiskPatternType.COMPLEXITY, 1)\n+]\n\\ No newline at end of file\n```\n\n\nGenerate specific test cases addressing the identified risk factors.\nFocus on security, compatibility, and error handling.\n\nFormat each test case exactly as follows:\n\nTC-001:\n- Title: [Specific test objective]\n- Priority: [Based on risk level]\n- Description: [Detailed scenario]\n- Steps:\n  - [Clear, actionable step]\n  - [Expected interaction]\n- Expected Results: [Verifiable outcome]\n\nGenerate at least 3 test cases.\n\n', 'options': {'temperature': 0.7}, 'stream': False}, 'api_base': 'http://localhost:11434/api/generate', 'headers': {}}
2025-01-28 14:50:17,072 - LiteLLM - DEBUG - PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {'model': 'mistral', 'prompt': '### User:\nGiven the following pull request changes:\n\nTitle: Feat risk analysis\nDescription: \n\nRisk Assessment:\n- Level: High\n- Factors: - Analysis Error: expected string or bytes-like object, got \'NoneType\'\n\nChanged Files:\nAdded:\n  - ABOUT.md\n  - config.yaml\n  - src/config.yaml\n  - src/utils/risk_analyzer.py\n  - src/utils/risk_patterns.py\nModified:\n  - src/core/factories.py\n  - src/core/registry.py\n  - src/core/test_case_generator.py\n  - src/main.py\n  - src/models/test_case.py\n  - src/pr_test_cases.yaml\n  - src/prompts/pr_test_case_prompt.txt\n  - src/prompts/templates/test_case.txt\n  - src/services/llm/llm_service.py\n  - src/utils/file_utils.py\n  - src/utils/risk_analysis.py\nRemoved:\n\nDiffs:\nFile: ABOUT.md\n```diff\n@@ -0,0 +1,133 @@\n+### **QitOps: Concept and Vision**\n+\n+**QitOps** is the practice of embedding quality assurance (QA) processes directly into the Git-based workflows of modern software development. It ensures that QA is not a siloed or afterthought activity but a continuous, collaborative, and automated part of the development lifecycle.\n+\n+At its core, QitOps leverages [GitOps](https://shalb.com/blog/gitops-an-introduction-to-gitops-principles-and-practices/) principlesâ€”treating everything as codeâ€”to integrate testing, risk analysis, and quality metrics into the workflows developers and QA engineers already use.\n+\n+---\n+\n+### **Core Tenets of QitOps**\n+\n+1. **QA 
as Code**:\n+    \n+    - Test cases, risk assessments, and quality gates are stored and managed as code in [Git repositories](https://www.simplilearn.com/tutorials/git-tutorial/what-is-a-git-repository).\n+    - [Test artifacts](https://artoftesting.com/test-artifacts-deliverables) (e.g., test cases, coverage reports) are versioned alongside application code.\n+2. **Seamless Integration**:\n+    \n+    - QA workflows are triggered automatically by Git events (e.g., [pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests), commits, merges).\n+    - QA becomes a natural part of the developer workflow, reducing handoffs and friction.\n+3. **Context-Aware Quality Assurance**:\n+    \n+    - QA processes adapt dynamically based on the context of code changes (e.g., [diffs](https://www.atlassian.com/git/tutorials/saving-changes/git-diff#:~:text=Diffing%20is%20a%20function%20that,%2C%20branches%2C%20files%20and%20more.), file types, PR descriptions).\n+    - Focused and relevant testing is prioritized, improving efficiency.\n+4. **Collaboration-Driven**:\n+    \n+    - QA is integrated into pull requests and [code reviews](https://about.gitlab.com/topics/version-control/what-is-code-review/), fostering collaboration between developers and QA engineers.\n+    - Teams can comment, refine, and approve test cases directly within PRs.\n+5. **Automation First**:\n+    \n+    - Automated pipelines handle test case generation, execution, and validation.\n+    - [Risk analysis](https://www.lambdatest.com/blog/how-to-incorporate-risk-management-strategy-in-testing/) and quality metrics are generated programmatically.\n+6. **Transparency and Metrics**:\n+    \n+    - QA workflows provide clear visibility into [quality metrics](https://www.testrail.com/qa-metrics/#glossary-of-qa-metric-formulas-5), such as test coverage, defect rates, and risk levels.\n+    - Metrics are made accessible to all stakeholders, enhancing accountability.\n+\n+---\n+\n+### **Key Components of QitOps**\n+\n+1. **GitHub Integration**:\n+    \n+    - QA workflows are tightly integrated with GitHub (or similar platforms, such as Bitbucket, Gitlab, CodeCommit etc) using APIs and GitHub Actions.\n+    - Test case generation and validation pipelines are triggered by PR events.\n+2. **AI-Powered Test Case Generation**:\n+    \n+    - Local or cloud-based LLMs analyze PRs, diffs, and descriptions to generate contextually relevant test cases.\n+    - AI assists with edge case identification, regression risks, and performance considerations.\n+3. **Dynamic Risk Assessment**:\n+    \n+    - Automated analysis of code changes to identify high-risk areas.\n+    - Risk levels influence the prioritization of test cases and QA workflows.\n+4. **Continuous Quality Feedback**:\n+    \n+    - PR comments, dashboards, and notifications provide real-time feedback on quality status.\n+    - Quality gates ensure critical issues are addressed before merging.\n+5. **Quality Pipelines**:\n+    \n+    - GitHub Actions or CI/CD tools execute automated tests based on generated cases.\n+    - Pipelines validate QA artifacts for completeness and consistency.\n+\n+---\n+\n+### **QitOps in Action**\n+\n+#### Example Workflow:\n+\n+1. **Developer Opens a PR**:\n+    \n+    - QitOps triggers an automated analysis of the PR (e.g., files changed, description, diff).\n+2. **Automated Test Case Generation**:\n+   
 \n+    - AI generates test cases based on the context of the changes.\n+    - Generated test cases are added as a comment or file in the PR.\n+3. **Risk Assessment and Metrics**:\n+    \n+    - The system evaluates the risk level of the changes and suggests areas requiring manual QA.\n+4. **Team Collaboration**:\n+    \n+    - QA engineers and developers review and refine test cases within the PR.\n+5. **Automated Validation**:\n+    \n+    - Pipelines execute tests and validate QA metrics, blocking the merge if quality gates are not met.\n+6. **Merge and Continuous Monitoring**:\n+    \n+    - Once all checks pass, the PR is merged, and QitOps logs the quality artifacts for future analysis.\n+\n+## QitOps Workflow\n+\n+```mermaid\n+flowchart TD\n+    A[Developer Opens Pull Request] --> B[QitOps Analyzes PR Context]\n+    B --> C[Risk Analysis Performed]\n+    C --> D[AI Generates Test Cases Based on Risk]\n+    D --> E[Generated Test Cases Added to PR]\n+    E --> F[QA and Dev Review & Refine]\n+    F --> G[Automated Tests Executed 
in CI/CD]\n+    G --> H{Quality Gates Passed?}\n+    H -->|Yes| I[Merge PR and Log QA Artifacts]\n+    H -->|No| J[Block Merge and Notify Team]\n+```\n+---\n+\n+### **Why QitOps Matters**\n+\n+1. **Accelerates Development**:\n+    \n+    - Embedding QA into Git workflows reduces bottlenecks and handoffs.\n+2. **Improves Quality**:\n+    \n+    - Automated, context-aware QA ensures high coverage and better defect detection.\n+3. **Encourages Collaboration**:\n+    \n+    - QA and development teams work closely through Git, fostering a shared responsibility for quality.\n+4. **Scales with Teams**:\n+    \n+    - QitOps adapts to the scale and complexity of modern software projects, making it suitable for teams of all sizes.\n+\n+---\n+\n+### **Future of QitOps**\n+\n+1. **Enhanced AI Capabilities**:\n+    - Use advanced NLP and diff analysis for deeper insights into changes.\n+2. **Multi-Tool Integration**:\n+    - Seamlessly work with tools like Jira, TestRail, and Slack.\n+3. **Community-Driven Standards**:\n+    - As an open-source initiative, QitOps could establish new standards for QA in DevOps workflows.\n+\n+---\n+\n+### Final Thoughts\n+\n+QitOps is not just a workflowâ€”itâ€™s a philosophy that redefines how QA fits into modern software development. \n\\ No newline at end of file\n```\n\nFile: config.yaml\n```diff\nNone\n```\n\nFile: src/config.yaml\n```diff\n@@ -0,0 +1,13 @@\n+providers:\n+  vcs:\n+    github:\n+      token: ${GITHUB_TOKEN}  # Will be loaded from environment\n+  llm:\n+    litellm:\n+      model: "ollama/mistral"\n+      temperature: 0.7\n+  output:\n+    yaml: {}\n+\n+prompt: "prompts/pr_test_case_prompt.txt"\n+output: "test_cases_output.yaml"\n\\ No newline at end of file\n```\n\nFile: src/core/factories.py\n```diff\n@@ -32,19 +32,23 @@ def configure(self, config: Dict[str, Any]) -> None:\n             if provider_type in [\'vcs\', \'llm\', \'output\']:\n
  registry = getattr(self, f"{provider_type}_registry")\n                 for name, cfg in provider_config.items():\n-                    # Register provider with its config in one step\n-                    if provider_type == \'vcs\' and name == \'github\':\n-                        from services.vcs.github_service import GitHubService\n-        
                registry.register(name, GitHubService, cfg)\n-                    elif provider_type == \'llm\' and name == \'litellm\':\n-                        from services.llm.llm_service import LLMService\n-                        registry.register(name, LLMService, cfg)\n-                    elif provider_type == \'output\' and name == \'yaml\':\n-                        from services.output.yaml_writer import YAMLWriter\n-                        registry.register(name, YAMLWriter, cfg)\n-                    elif provider_type == \'output\' and name == \'json\':\n-                        from services.output.json_writer import JSONWriter\n-                        registry.register(name, JSONWriter, cfg)\n+                    # Only register if not already registered\n+                    if name not in registry.list_providers():\n+                        if provider_type == \'vcs\' and name == \'github\':\n+                            from services.vcs.github_service import GitHubService\n+                            registry.register(name, GitHubService, cfg)\n+                        elif provider_type == \'llm\' and name == \'litellm\':\n+                            from services.llm.llm_service 
import LLMService\n+                            registry.register(name, LLMService, cfg)\n+                        elif provider_type == \'output\' and name == \'yaml\':\n+   
                         from services.output.yaml_writer import YAMLWriter\n+                            registry.register(name, YAMLWriter, cfg)\n+                        elif provider_type == \'output\' and name == \'json\':\n+                            from services.output.json_writer import JSONWriter\n+                            registry.register(name, JSONWriter, cfg)\n+                    else:\n+                        # Update existing provider config\n+                        registry.update_config(name, cfg)\n \n # Global factory manager instance\n factory_manager = FactoryManager()\n\\ No newline at end of file\n```\n\nFile: src/core/registry.py\n```diff\n@@ -1,4 +1,4 @@\n-from typing import Dict, Type, Any\n+from typing import Dict, Type, Any, List\n from services.base.vcs_provider import VCSProvider\n from services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n@@ -15,23 +15,31 @@ def register(self, name: str, provider: Type, config: Dict[str, Any] = None) ->\n        
 if not name or not isinstance(name, str):\n             raise ValueError("Provider name must be a non-empty string")\n         \n-        if name in self._providers:\n-      
      raise ValueError(f"Provider \'{name}\' already registered")\n-            \n-        self._providers[name] = provider\n-        if config:\n-            self._configs[name] = config\n+        if name not in self._providers:\n+            self._providers[name] = provider\n+            if config:\n+                self._configs[name] = config\n+        else:\n+            # Update config if provider exists\n+            self.update_config(name, config)\n     \n-    def get_provider(self, name: str) -> Type:\n-      
  """Get a provider implementation by name."""\n+    def update_config(self, name: str, config: Dict[str, Any]) -> None:\n+        """Update configuration for existing provider"""\n         if name not in self._providers:\n             raise KeyError(f"Provider \'{name}\' not found")\n-        return self._providers[name]\n-    \n+        if isinstance(config, dict):\n+            self._configs[name] = config\n+        else:\n+            raise ValueError(f"Config must be a dictionary, got {type(config)}")\n+\n+    def 
get_provider(self, name: str) -> Type:\n+        """Get a provider implementation by name."""\n+        return self._providers.get(name)\n+\n     def get_config(self, name: str) -> Dict[str, Any]:\n         """Get provider configuration by name."""\n         return self._configs.get(name, {})\n-    \n-    def list_providers(self) -> Dict[str, Type]:\n+\n+    def list_providers(self) -> List[str]:\n         """List all registered providers."""\n-        return self._providers.copy()\n\\ No newline at end of file\n+      
  return list(self._providers.keys())\n\\ No newline at end of file\n```\n\nFile: src/core/test_case_generator.py\n```diff\n@@ -1,13 +1,15 @@\n+from datetime import datetime\n from services.base.vcs_provider import VCSProvider\n from services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n from models.test_case import TestCase\n from models.pull_request import PullRequest\n-from utils.risk_analysis import analyze_diff\n+from utils.risk_analyzer import RiskAnalyzer\n from rich.console import Console\n from rich.panel import Panel\n+from rich.table import Table\n import yaml\n-from typing import List, Dict\n+from typing import List, Dict, Any\n import re\n import logging\n \n@@ -19,53 +21,101 @@ def __init__(self,\n         self.vcs_provider = vcs_provider\n         self.llm_provider = llm_provider\n         self.output_provider = output_provider\n+        self.risk_analyzer = RiskAnalyzer()\n         self.console = Console()\n+        self.logger = logging.getLogger(__name__)\n \n     def generate(self, repo: str, pr_number: int, output_file: str) -> None:\n         self.console.print(f"[bold blue]ðŸš€ Generating test cases for PR #{pr_number}[/bold blue]")\n 
        \n+        with self.console.status("[bold yellow]Analyzing PR...") as status:\n+            try:\n+                pr = self.vcs_provider.get_pull_request(repo, pr_number)\n+                risk_analysis = self._analyze_risk(pr)\n+                self._display_risk_analysis(risk_analysis)\n+                \n+                status.update("[bold yellow]Generating test cases...")\n+                prompt = self._load_prompt()\n+                context = self._create_context(pr, risk_analysis)\n+                \n+                llm_output = self.llm_provider.generate(prompt, context)\n+                test_cases = self._parse_test_cases(llm_output)\n+                \n+
   if not test_cases:\n+                    self.console.print("[red]Warning: No test cases were generated[/red]")\n+                \n+                self._save_results(pr, 
risk_analysis, test_cases, output_file)\n+                self.console.print(f"\\n[green]âœ… Results saved to {output_file}[/green]")\n+                \n+            except Exc
eption as e:\n+                self.logger.error(f"Generation error: {str(e)}")\n+                self.console.print(f"[red]Error: {str(e)}[/red]")\n+                raise\n+\n+    def _analyze_risk(self, pr: PullRequest) -> Dict[str, Any]:\n+        return self.risk_analyzer.analyze(pr.changes, pr.diffs)\n+\n+    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n         try:\n-            pr = self.vcs_provider.get_pull_request(repo, pr_number)\n-            risk_analysis = self._analyze_risk(pr)\n-            \n-            # Load and format prompt\n-            prompt = self._load_prompt()\n-            context = self._create_context(pr, risk_analysis)\n-
   \n-            # Generate test cases\n-            self.console.print("[yellow]Generating test cases from LLM...[/yellow]")\n-            llm_output = self.llm_provider.generate(prompt, context)\n-            logging.debug(f"LLM Output: {llm_output}")\n-            \n-            # Parse test cases\n-            test_cases = self._parse_test_cases(llm_output)\n-            if not test_cases:\n-                self.console.print("[red]Warning: No test cases were generated[/red]")\n+            # Ensure risk_analysis is a dict\n+            if not isinstance(risk_analysis, dict):\n+                self.logger.warning(f"Invalid risk_analysis type: {type(risk_analysis)}")\n+                risk_analysis = {\n+                    "level": "Unknown",\n+                    "factors": [],\n+                    "details": []\n+                }\n             \n-       
     self._save_results(pr, risk_analysis, test_cases, output_file)\n-            self.console.print(f"\\n[green]âœ… Results saved to {output_file}[/green]")\n+            # For
mat risk factors safely using \'factors\' instead of \'risk_factors\'\n+            factors = risk_analysis.get("factors", [])\n+            risk_factors = ", ".join(str(f) for f in factors) if factors else "None"\n             \n+            return {\n+                "pr_title": pr.title,\n+                "pr_description": pr.description,\n+    
            "risk_level": risk_analysis.get("level", "Unknown"),\n+                "risk_factors": risk_factors,\n+                "changes": self._format_changes(pr.changes),\n+                "diffs": self._format_diffs(pr.diffs)\n+            }\n         except Exception as e:\n-            self.console.print(f"[red]Error: {str(e)}[/red]")\n+   
         self.logger.error(f"Error creating context: {e}")\n             raise\n \n-    def _analyze_risk(self, pr: PullRequest) -> dict:\n-        return {\n-            "level": "High" if len(pr.changes["modified"]) > 5 else "Medium",\n-            "factors": [analyze_diff(diff) for diff in pr.diffs.values()]\n-        }\n+    def _format_changes(self, changes: Dict[str, Any]) -> str:\n+        try:\n+            if not isinstance(changes, dict):\n+                return str(changes)\n+            \n+            formatted = []\n+            for change_type, files in changes.items():\n+                if isinstance(files, list):\n+                    formatted.append(f"{change_type.capitalize()}:")\n+                    formatted.extend(f"  - {file}" for file in files)\n+                else:\n+                    formatted.append(f"{change_type}: {files}")\n+  
          return "\\n".join(formatted)\n+        except Exception as e:\n+            self.logger.error(f"Error formatting changes: {e}")\n+            return str(changes)\n \n-    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n-        return {\n-            "pr": {\n-                "number": pr.number,\n-
  "title": pr.title,\n-                "description": pr.description,\n-                "changes": pr.changes,\n-                "diffs": pr.diffs\n-            },\n-
   "risk_analysis": risk_analysis\n-        }\n+    def _format_diffs(self, diffs: Dict[str, Any]) -> str:\n+        try:\n+            if not isinstance(diffs, dict):\n+     
           return str(diffs)\n+                \n+            formatted = []\n+            for file_path, diff in diffs.items():\n+                formatted.extend([\n+       
             f"File: {file_path}",\n+                    "```diff",\n+                    str(diff).strip(),\n+                    "```\\n"\n+                ])\n+
return "\\n".join(formatted)\n+        except Exception as e:\n+            self.logger.error(f"Error formatting diffs: {e}")\n+            return str(diffs)\n \n     def _load_prompt(self) -> str:\n         with open(\'prompts/templates/test_case.txt\', \'r\') as f:\n@@ -77,51 +127,84 @@ def _parse_test_cases(self, llm_output: str) -> List[Dict]:\n             # Split into individual test cases\n             case_blocks = re.split(r\'TC-\\d+:\', llm_output)\n             if not case_blocks[0].strip():\n-
 case_blocks = case_blocks[1:]  # Remove empty first split\n+                case_blocks = case_blocks[1:]\n                 \n             for i, block in enumerate(case_blocks, 1):\n                 if not block.strip():\n                     continue\n                     \n-                # Extract fields using multiline patterns\n-
     title_match = re.search(r\'Title:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-                priority_match = re.search(r\'Priority:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-                desc_match = re.search(r\'Description:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n+                # Extract fields with improved regex patterns\n+                title_match = re.search(r\'Title:\\s*([^\\n]+)\', block)\n+                priority_match = re.search(r\'Priority:\\s*([^\\n]+)\', block)\n+
  description_match = re.search(r\'Description:\\s*([^\\n]+)\', block)\n                 \n                 # Extract steps as list\n                 steps = []\n-
    steps_section = re.search(r\'Steps:(.*?)(?=Expected Results:|\\Z)\', block, re.DOTALL)\n-                if steps_section:\n+                steps_match = re.search(r\'Steps:(.*?)(?=Expected Results:|$)\', block, re.DOTALL)\n+                if steps_match:\n                     steps = [\n-                        s.strip().lstrip(\'- \') \n-  
                      for s in steps_section.group(1).strip().split(\'\\n\')\n-                        if s.strip() and s.strip().startswith(\'-\')\n+                        s.strip()[2:] for s in steps_match.group(1).splitlines() \n+                        if s.strip().startswith(\'-\')\n                     ]\n                 \n-
# Extract expected results\n-                expected_match = re.search(r\'Expected Results:\\s*(.*?)(?=\\n\\n|\\Z)\', block, re.DOTALL)\n+                expected_match = re.search(r\'Expected Results:\\s*([^\\n]+(?:\\n(?!\\n).*)*)\', block, re.DOTALL)\n+                \n+                self.logger.debug(f"Title match: {title_match.group(1) if title_match else \'No match\'}")\n                 \n-                test_case = {\n+                test_cases.append({\n                     "id": f"TC-{i:03d}",\n
           "title": title_match.group(1).strip() if title_match else "No title",\n                     "priority": priority_match.group(1).strip() if priority_match else "Medium",\n-                    "description": desc_match.group(1).strip() if desc_match else "No description",\n+                    "description": description_match.group(1).strip() if description_match else "No description",\n                     "steps": steps,\n-                    "expected_result": expected_match.group(1).strip() if expected_match else "No expected results"\n-                }\n-                test_cases.append(test_case)\n+                    "expected_result": expected_match.group(1).strip() if expected_match else "No expected results",\n+                    "generated_at": datetime.now().isoformat(),\n+                    "approved": False,\n+                    "approved_by": None\n+                })\n                 \n         except Exception as e:\n             self.logger.error(f"Error parsing test cases: {str(e)}")\n             self.logger.debug(f"Raw LLM output:\\n{llm_output}")\n-        \n+            \n         return test_cases\n \n+    def _extract_field(self, text: str, pattern: str, default: str, flags: re.RegexFlag = 0) -> str:\n+        match = re.search(pattern, text, flags)\n+        return match.group(1).strip() if match else default\n+\n     def _save_results(self, pr: PullRequest, risk_analysis: dict, test_cases: List[Dict], output_file: str) -> None:\n         results = {\n             "pr_number": pr.number,\n             "pr_title": pr.title,\n             "risk_analysis": risk_analysis,\n             "test_cases": test_cases  # Already in dict format\n         }\n-        self.output_provider.write(results, output_file)\n\\ No newline at end of file\n+        self.output_provider.write(results, output_file)\n+\n+    def _display_risk_analysis(self, risk_analysis: Dict[str, Any]) -> None:\n+        table = Table(title="Risk Analysis Results")\n+        \n+        table.add_column("Category", style="cyan")\n+        table.add_column("Details", 
style="magenta")\n+        \n+        # Safely get risk level\n+        level = risk_analysis.get(\'level\', \'Unknown\')\n+        table.add_row("Risk Level", f"[bold]{level}[/bold]")\n+        \n+        # Safely handle factors and details\n+        factors = risk_analysis.get(\'factors\', [])\n+        details = risk_analysis.get(\'details\', [])\n+        \n+        # If we have both factors and details, zip them\n+        if factors and details:\n+            for factor, detail in zip(factors, details):\n+
       table.add_row(str(factor), str(detail))\n+        # Otherwise just show factors\n+        elif factors:\n+            for factor in factors:\n+                table.add_row(str(factor), "")\n+        \n+        self.console.print("\\n")\n+        self.console.print(table)\n+        self.console.print("\\n")\n\\ No newline at end of file\n```\n\nFile: src/main.py\n```diff\n@@ -1,29 +1,47 @@\n from core.factories import factory_manager\n from core.test_case_generator import TestCaseGenerator\n from utils.file_utils import load_config\n+import logging\n+import sys\n+import os\n import argparse\n \n def main():\n-    parser = argparse.ArgumentParser(\n-        description=\'Generate test 
cases from PRs\'\n+    logging.basicConfig(\n+        level=logging.DEBUG,\n+        format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n     )\n-    parser.add_argument(\'repo\')\n-    parser.add_argument(\'pr_number\', type=int)\n-    parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n-    parser.add_argument(\'--config\', default=\'config.yaml\')\n-    args = parser.parse_args()\n+    logger = logging.getLogger(__name__)\n \n-    config = load_config(args.config)\n-    factory_manager.configure(config)\n+    try:\n+        config_path = os.path.join(os.path.dirname(__file__), \'config.yaml\')\n+        config = load_config(config_path)\n+        \n+        factory_manager.configure(config)\n \n-    vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n-    llm = factory_manager.llm_factory.create("litellm", \n-                                             model=config["providers"]["llm"]["litellm"]["model"], \n-
  temperature=config["providers"]["llm"]["litellm"]["temperature"])\n-    output = factory_manager.output_factory.create("yaml")\n+        parser = argparse.ArgumentParser(\n+            description=\'Generate test cases from PRs\'\n+        )\n+        parser.add_argument(\'repo\')\n+        parser.add_argument(\'pr_number\', type=int)\n+        parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n+        parser.add_argument(\'--config\', default=\'config.yaml\')\n+        args = parser.parse_args()\n \n-    generator = TestCaseGenerator(vcs, llm, output)\n-    generator.generate(args.repo, args.pr_number, args.output)\n+        config = load_config(args.config)\n+        factory_manager.configure(config)\n+\n+        vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n+        llm = factory_manager.llm_factory.create("litellm", \n+                                                 model=config["providers"]["llm"]["litellm"]["model"], \n+
                  temperature=config["providers"]["llm"]["litellm"]["temperature"])\n+        output = factory_manager.output_factory.create("yaml")\n+\n+        generator = TestCaseGenerator(vcs, llm, output)\n+        generator.generate(args.repo, args.pr_number, args.output)\n+    except Exception as e:\n+        logger.error(f"Failed to initialize: {str(e)}")\n+        sys.exit(1)\n \n if __name__ == "__main__":\n     main()\n\\ No newline at end of file\n```\n\nFile: src/models/test_case.py\n```diff\n@@ -1,5 +1,6 @@\n from dataclasses import dataclass\n-from typing import List\n+from typing import List, Optional\n+from datetime import datetime\n \n @dataclass\n class TestCase:\n@@ -8,4 
+9,8 @@ class TestCase:\n     priority: str\n     description: str\n     steps: List[str]\n-    expected_result: str\n\\ No newline at end of file\n+    expected_result: str\n+    generated_at: datetime = datetime.now()\n+    approved: bool = False\n+    approved_by: Optional[str] = None\n+    risk_factors: List[str] = None\n\\ No newline at end of file\n```\n\nFile: src/pr_test_cases.yaml\n```diff\n@@ -1,53 +1,71 @@\n pr_number: 1\n pr_title: Bump starlette from 0.27.0 to 0.40.0 in /src\n risk_analysis:\n-  level: Medium\n+  level: High\n   factors:\n-  - []\n+  - Security Risk\n+  - Dependency Changes\n+  details:\n+  - Security-sensitive code changes detected\n+  - Package dependencies modified\n test_cases:\n - id: TC-001\n-  title: No title\n-  priority: Medium\n-  description: No description\n+  title: Verify the security of the updated Starlette dependency in the application\n+  priority: High\n+  description: Check if any newly introduced vulnerabilities or security risks are\n+    present in the updated version (0.40.0) of Starlette, which could potentially\n+    impact the overall security of the application.\n   steps:\n-  - Import starlette module in a test script\n-  - Write a test function which was working with the previous version and check if\n-    it works as expected with the new version\n+  - Install the updated version of Starlette (0.40.0) in a separate development environment\n+  - Run a comprehensive vulnerability scan on the isolated environment using tools\n+    like OWASP ZAP or Bandit\n+  - Check for any reported vulnerabilities, security issues, or warnings related to\n+    the updated Starlette package\n   - \'\'\n-  expected_result: The test function should pass without any errors or unexpected\n-    behaviors.\n+  expected_result: The security scan should not reveal any critical or high-severity\n+    vulnerabilities, ensuring that the application remains secure. If any issues are\n+    found, they must be addressed and fixed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.979267\'\n+  approved: false\n+  approved_by: null\n - id: TC-002\n-  title: No title\n+  title: Ensure compatibility of the updated Starlette dependency with other dependencies\n+    in the application\n   priority: Medium\n-  description: No description\n+  description: Validate that the updated version (0.40.0) of Starlette is compatible\n+    with the existing versions of other dependencies within the application, to prevent\n+    any potential conflicts or errors during runtime.\n   steps:\n-  - Install and run OWASP ZAP or bandit on the new starlette codebase\n-  - Analyze the results for any potential security vulnerabilities\n+  - Create a development environment containing all the dependencies listed in the\n+    requirements.txt file (excluding Starlette for now)\n+  - Install the updated version of Starlette (0.40.0) in this environment\n+  - Run the application to check if there are any errors, warnings, or unexpected\n+    behavior caused by the updated Starlette dependency\n   - \'\'\n-  expected_result: The test should not find any critical or high severity security\n-    vulnerabilities in the new version.\n+  expected_result: The application should run smoothly without any critical errors\n+    or warning messages related to the updated Starlette package, demonstrating compatibility\n+    with other dependencies. If any issues occur, they must be resolved before merging\n+    this update.\n+  generated_at: \'2025-01-28T14:27:14.979975\'\n+  approved: false\n+  approved_by: null\n - id: TC-003\n-  title: No title\n-  priority: Medium\n-  description: No description\n-  steps:\n-  - Create a custom exception class\n-  - Use the custom exception in a route handler and trigger it intentionally\n-  - Check if the exception is handled correctly by starlette and an appropriate error\n-    message is returned to the client\n-  - \'\'\n-  expected_result: Starlette should be able to handle the custom exception gracefully,\n-    returning an appropriate error message to the client.\n-- id: TC-004\n-  title: No title\n+  title: Test error handling of the updated Starlette dependency in the application\n   priority: Medium\n-  description: No description\n+  description: Verify that the updated version (0.40.0) of Starlette handles errors\n+    and exceptions gracefully, ensuring a good user experience even when unexpected\n+    situations occur.\n   steps:\n-  - Create a test script which uses an API or functionality from starlette 0.27.0\n-  - Run the test script with both versions (0.27.0 and 0.40.0) of starlette installed\n-    separately\n-  - Check if there are any differences in behavior or outcomes between the two versions\n+  - Create test scenarios where exceptional conditions are intentionally triggered\n+    within the application (e.g., by providing invalid inputs or forcing certain edge\n+    cases)\n+  - Observe how the updated Starlette dependency handles these exceptions and errors,\n+    checking for appropriate logging, error messages, and recovery mechanisms\n   - \'\'\n-  expected_result: There should not be any noticeable differences in behavior or outcomes\n-    when using the same API or functionality with both versions (0.27.0 and 0.40.0)\n-    of starlette.\n+  expected_result: The updated Starlette dependency should handle errors effectively,\n+    providing descriptive error messages to help troubleshoot issues and maintaining\n+    application stability in the event of exceptional conditions. If any issues occur,\n+    they must be addressed 
before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.980992\'\n+  approved: false\n+  approved_by: null\n```\n\nFile: src/prompts/pr_test_case_prompt.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure 
and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ 
No newline at end of file\n```\n\nFile: src/prompts/templates/test_case.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - 
[Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/services/llm/llm_service.py\n```diff\n@@ -27,29 +27,21 @@ def generate(self, prompt: str, context: Dict[str, Any]) -> str:\n         return result\n \n     def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:\n-        pr = context.get(\'pr\', {})\n-        risk = context.get(\'risk_analysis\', {})\n-        \n-        # Format changes\n-        changes_str = "\\n".join(\n-     
       f"{k}: {\', \'.join(v)}" \n-            for k, v in pr.get(\'changes\', {}).items() \n-            if v\n-        )\n-        \n-        # Format diffs\n-        diffs_str = "\\n".join(\n-            f"File: {fname}\\n{diff}" \n-            for fname, diff in pr.get(\'diffs\', {}).items()\n-        )\n-        \n-        return prompt.format(\n-            pr_title=pr.get(\'title\', \'\'),\n-            pr_description=pr.get(\'description\', \'\'),\n-            risk_level=risk.get(\'level\', \'\'),\n-
 changes=changes_str,\n-            diffs=diffs_str\n-        )\n+        try:\n+            return prompt.format(\n+                pr_title=context.get(\'pr_title\', \'\'),\n+                pr_description=context.get(\'pr_description\', \'\'),\n+                risk_level=context.get(\'risk_level\', \'\'),\n+                risk_factors=context.get(\'risk_factors\', \'\'),\n+                changes=context.get(\'changes\', \'\'),\n+                diffs=context.get(\'diffs\', \'\')\n+            )\n+        except KeyError as e:\n+            self.logger.error(f"Missing required key in context: {e}")\n+            raise\n+        except Exception as e:\n+            self.logger.error(f"Error formatting prompt: {e}")\n+            raise\n \n     def _format_changes(self, changes: Dict[str, Any]) -> str:\n         result = []\n```\n\nFile: src/utils/file_utils.py\n```diff\n@@ -1,5 +1,40 @@\n+import os\n import yaml\n+import logging\n+from typing import Dict, Any\n+import re\n \n-def load_config(path: str) -> dict:\n-    with open(path, \'r\', encoding=\'utf-8\') as f:\n-        return yaml.safe_load(f)\n\\ No newline at end of file\n+def load_config(path: str) -> Dict[str, Any]:\n+    logger = logging.getLogger(__name__)\n+    \n+    if not os.path.exists(path):\n+        logger.error(f"Config file not found: {path}")\n+        raise FileNotFoundError(f"Config file not found: 
{path}")\n+        \n+    with open(path, \'r\') as f:\n+        config = yaml.safe_load(f)\n+        \n+    if not config:\n+        raise ValueError("Empty config file")\n+ 
       \n+    # Replace environment variables\n+    config = _replace_env_vars(config)\n+    \n+    return config\n+\n+def _replace_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:\n+    """Recursively replace ${VAR} with environment variable values"""\n+    if isinstance(config, dict):\n+        return {k: _replace_env_vars(v) for k, v in config.items()}\n+    elif isinstance(config, list):\n+        return [_replace_env_vars(v) for v in config]\n+    elif isinstance(config, str):\n+        pattern = r\'\\${([^}]+)}\'\n+        match = re.search(pattern, config)\n+        if match:\n+            env_var = match.group(1)\n+            env_value = os.getenv(env_var)\n+            if not env_value:\n+                raise ValueError(f"Environment variable {env_var} not set")\n+            return config.replace(match.group(0), env_value)\n+    return config\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analysis.py\n```diff\n@@ -1,24 +1,45 @@\n from enum import Enum\n+from typing import Dict, List, Any\n+import re\n \n class RiskLevel(Enum):\n     LOW = "Low"\n     MEDIUM = "Medium"\n     HIGH = "High"\n \n-def analyze_diff(diff_content):\n-    """Analyze diff content for risk patterns."""\n-  
  risk_patterns = {\n-        "sql": "Database changes",\n-        "await": "Async flow changes",\n-        "try": "Error handling changes",\n-        "import": "Dependency changes",\n-        "class": "Class structure changes",\n-        "function": "Function signature changes"\n-    }\n+class RiskFactor(Enum):\n+    DEPENDENCY = "Dependency Changes"\n+    SECURITY = "Security Impact"\n+    BREAKING = "Breaking Changes"\n+    COVERAGE = "Test Coverage"\n \n-    found_patterns = []\n-    for pattern, risk in risk_patterns.items():\n-        if pattern in diff_content.lower():\n-            found_patterns.append(risk)\n-\n-    return found_patterns\n\\ No newline at end of file\n+def analyze_risk(changes: Dict[str, List[str]], diffs: Dict[str, str]) -> Dict[str, Any]:\n+    risk_factors = []\n+    \n+    # Check dependency changes\n+    if any(\'requirements.txt\' in file or \'package.json\' in file for file in changes.get(\'modified\', [])):\n+        risk_factors.append(RiskFactor.DEPENDENCY.value)\n+    \n+    # Check security patterns\n+    security_patterns = [\n+        r\'auth[orization]*\',\n+        r\'password\',\n+        r\'secret\',\n+        r\'token\',\n+        r\'crypt\'\n+    ]\n+    \n+  
  if any(re.search(\'|\'.join(security_patterns), diff, re.I) for diff in diffs.values()):\n+        risk_factors.append(RiskFactor.SECURITY.value)\n+    \n+    # Determine risk level\n+    risk_level = RiskLevel.LOW\n+    if len(risk_factors) >= 2:\n+        risk_level = RiskLevel.HIGH\n+    elif len(risk_factors) == 1:\n+        risk_level = RiskLevel.MEDIUM\n+    \n+    return {\n+        "level": risk_level.value,\n+        "factors": risk_factors\n+    }\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analyzer.py\n```diff\n@@ -0,0 +1,85 @@\n+from typing import Dict, List, Any, Union\n+from enum import Enum\n+import re\n+import logging\n+\n+class RiskLevel(Enum):\n+    LOW = "Low"\n+    MEDIUM = "Medium"\n+    HIGH = "High"\n+\n+class RiskAnalyzer:\n+    def __init__(self):\n+        self.logger = logging.getLogger(__name__)\n+        self.security_patterns = [\n+            r\'auth\\w*\',\n+            r\'password\',\n+            r\'secret\',\n+            r\'token\',\n+            r\'crypt\'\n+        ]\n+        self.breaking_patterns = [\n+            r\'break.*change\',\n+            r\'deprecat\\w*\',\n+            r\'remov\\w+\\s+\\w+\',\n+            r\'delet\\w+\\s+\\w+\'\n+       
 ]\n+\n+    def analyze(self, changes: Union[Dict[str, List[str]], str], diffs: Union[Dict[str, str], str]) -> Dict[str, Any]:\n+        try:\n+            # Normalize inputs\n+            changes_dict = changes if isinstance(changes, dict) else {"modified": [str(changes)]}\n+            diffs_dict = diffs if isinstance(diffs, dict) else {"file": str(diffs)}\n+            \n+            risk_factors = []\n+            details = []\n+\n+            # Check security risks\n+            if self._check_security_risks(diffs_dict):\n+                risk_factors.append("Security Risk")\n+                details.append("Security-sensitive code changes detected")\n+\n+            # Check dependency 
changes\n+            if self._check_dependency_changes(changes_dict):\n+                risk_factors.append("Dependency Changes")\n+                details.append("Package dependencies modified")\n+\n+            # Check breaking changes\n+            if self._check_breaking_changes(diffs_dict):\n+                risk_factors.append("Breaking Changes")\n+                details.append("Breaking changes detected")\n+\n+            level = self._determine_risk_level(risk_factors)\n+            \n+            return {\n+ 
               "level": level,\n+                "factors": risk_factors,\n+                "details": details\n+            }\n+        except Exception as e:\n+            self.logger.error(f"Error in risk analysis: {e}")\n+            return {\n+                "level": "High",\n+                "factors": ["Analysis Error"],\n+                "details": [str(e)]\n+            }\n+\n+    def _check_security_risks(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.security_patterns)\n+            for content in diffs.values()\n+        )\n+\n+    def _check_dependency_changes(self, changes: Dict[str, List[str]]) -> bool:\n+        dependency_files = [\'requirements.txt\', \'package.json\', \'build.gradle\', \'pom.xml\']\n+        modified = changes.get(\'modified\', [])\n+        return any(dep in str(file) for dep in dependency_files for file in modified)\n+\n+    def _check_breaking_changes(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.breaking_patterns)\n+            for content in diffs.values()\n+        )\n+\n+    def _determine_risk_level(self, factors: List[str]) -> str:\n+        return "High" if len(factors) >= 2 else "Medium" if factors else "Low"\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_patterns.py\n```diff\n@@ -0,0 +1,22 @@\n+from enum import Enum\n+from typing import List, Pattern\n+import re\n+\n+class RiskPatternType(Enum):\n+    SECURITY = "security"\n+    BREAKING = "breaking"\n+    PERFORMANCE = "performance"\n+    COMPLEXITY = "complexity"\n+\n+class RiskPattern:\n+    def __init__(self, pattern: str, risk_type: RiskPatternType, weight: int = 1):\n+        self.pattern = re.compile(pattern, re.IGNORECASE)\n+        self.type = risk_type\n+        self.weight = weight\n+\n+RISK_PATTERNS = [\n+    RiskPattern(r\'auth|login|password|secret|token\', RiskPatternType.SECURITY, 3),\n+    RiskPattern(r\'break.*change|deprecat|remove[d]?\\s+\\w+\', RiskPatternType.BREAKING, 2),\n+    RiskPattern(r\'performance|optimize|slow|fast\', RiskPatternType.PERFORMANCE, 2),\n+    RiskPattern(r\'complex|complicated|confusing\', RiskPatternType.COMPLEXITY, 1)\n+]\n\\ No newline at end of file\n```\n\n\nGenerate specific test cases addressing the identified risk factors.\nFocus on security, compatibility, and error handling.\n\nFormat each test case exactly as follows:\n\nTC-001:\n- Title: [Specific test objective]\n- Priority: [Based on risk level]\n- Description: [Detailed scenario]\n- Steps:\n 
 - [Clear, actionable step]\n  - [Expected interaction]\n- Expected Results: [Verifiable outcome]\n\nGenerate at least 3 test cases.\n\n', 'options': {'temperature': 0.7}, 'stream': False}, 'api_base': 'http://localhost:11434/api/generate', 'headers': {}}
14:50:17 - LiteLLM:DEBUG: litellm_logging.py:624 -

POST Request Sent from LiteLLM:
curl -X POST \
http://localhost:11434/api/generate \
-d '{'model': 'mistral', 'prompt': '### User:\nGiven the following pull request changes:\n\nTitle: Feat risk analysis\nDescription: \n\nRisk Assessment:\n- Level: High\n- Factors: - Analysis Error: expected string or bytes-like object, got \'NoneType\'\n\nChanged Files:\nAdded:\n  - ABOUT.md\n  - config.yaml\n  - src/config.yaml\n  - src/utils/risk_analyzer.py\n  - src/utils/risk_patterns.py\nModified:\n  - src/core/factories.py\n  - src/core/registry.py\n  - src/core/test_case_generator.py\n  - src/main.py\n  - src/models/test_case.py\n  - src/pr_test_cases.yaml\n  - src/prompts/pr_test_case_prompt.txt\n  - src/prompts/templates/test_case.txt\n  - src/services/llm/llm_service.py\n  - src/utils/file_utils.py\n  - src/utils/risk_analysis.py\nRemoved:\n\nDiffs:\nFile: ABOUT.md\n```diff\n@@ -0,0 +1,133 @@\n+### **QitOps: Concept and Vision**\n+\n+**QitOps** is the practice of embedding quality assurance (QA) processes directly into the Git-based workflows of modern software development. It ensures that QA is not a siloed or afterthought 
activity but a continuous, collaborative, and automated part of the development lifecycle.\n+\n+At its core, QitOps leverages [GitOps](https://shalb.com/blog/gitops-an-introduction-to-gitops-principles-and-practices/) principlesâ€”treating everything as codeâ€”to integrate testing, risk analysis, and quality metrics into the workflows developers and QA engineers already use.\n+\n+---\n+\n+### **Core Tenets of QitOps**\n+\n+1. **QA as Code**:\n+    \n+    - Test cases, risk assessments, and quality gates are stored and managed as code in [Git repositories](https://www.simplilearn.com/tutorials/git-tutorial/what-is-a-git-repository).\n+    - [Test artifacts](https://artoftesting.com/test-artifacts-deliverables) (e.g., test cases, coverage reports) are versioned alongside application code.\n+2. **Seamless Integration**:\n+    \n+    - QA workflows are triggered automatically by Git events (e.g., [pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests), commits, merges).\n+    - QA becomes a natural part of the developer workflow, reducing handoffs and friction.\n+3. **Context-Aware Quality Assurance**:\n+    \n+    - QA processes adapt dynamically based on the context of code changes (e.g., [diffs](https://www.atlassian.com/git/tutorials/saving-changes/git-diff#:~:text=Diffing%20is%20a%20function%20that,%2C%20branches%2C%20files%20and%20more.), file types, PR descriptions).\n+    - Focused and relevant testing is prioritized, improving efficiency.\n+4. **Collaboration-Driven**:\n+    \n+    - QA is integrated into pull requests and [code reviews](https://about.gitlab.com/topics/version-control/what-is-code-review/), fostering collaboration between developers and QA engineers.\n+    - Teams can comment, refine, and approve test cases directly within PRs.\n+5. **Automation First**:\n+    \n+    - Automated pipelines handle test case generation, execution, and validation.\n+    - [Risk analysis](https://www.lambdatest.com/blog/how-to-incorporate-risk-management-strategy-in-testing/) and quality metrics are generated programmatically.\n+6. **Transparency and Metrics**:\n+    \n+    - QA workflows provide clear visibility into [quality metrics](https://www.testrail.com/qa-metrics/#glossary-of-qa-metric-formulas-5), such as test coverage, defect rates, and risk levels.\n+    - Metrics are made accessible to all stakeholders, enhancing accountability.\n+\n+---\n+\n+### **Key Components of QitOps**\n+\n+1. **GitHub Integration**:\n+    \n+    - QA workflows are tightly integrated with GitHub (or similar platforms, such as Bitbucket, Gitlab, CodeCommit etc) using APIs and GitHub Actions.\n+    - Test case generation and validation pipelines are triggered by PR events.\n+2. **AI-Powered Test Case Generation**:\n+    \n+    - Local or cloud-based LLMs analyze PRs, diffs, and descriptions to generate contextually relevant test cases.\n+  
  - AI assists with edge case identification, regression risks, and performance considerations.\n+3. **Dynamic Risk Assessment**:\n+    \n+    - Automated analysis of code changes to identify high-risk areas.\n+    - Risk levels influence the prioritization of test cases and QA workflows.\n+4. **Continuous Quality Feedback**:\n+    \n+    - PR comments, dashboards, and notifications provide real-time feedback on quality status.\n+    - Quality gates ensure critical issues are addressed before merging.\n+5. **Quality Pipelines**:\n+    \n+    - GitHub Actions or CI/CD tools execute automated tests based on generated cases.\n+    - Pipelines validate QA artifacts for completeness and consistency.\n+\n+---\n+\n+### **QitOps in Action**\n+\n+#### Example Workflow:\n+\n+1. **Developer Opens a PR**:\n+    \n+    - QitOps triggers an automated analysis of the PR (e.g., 
files changed, description, diff).\n+2. **Automated Test Case Generation**:\n+    \n+    - AI generates test cases based on the context of the changes.\n+    - Generated test 
cases are added as a comment or file in the PR.\n+3. **Risk Assessment and Metrics**:\n+    \n+    - The system evaluates the risk level of the changes and suggests areas requiring manual QA.\n+4. **Team Collaboration**:\n+    \n+    - QA engineers and developers review and refine test cases within the PR.\n+5. **Automated Validation**:\n+    \n+  
  - Pipelines execute tests and validate QA metrics, blocking the merge if quality gates are not met.\n+6. **Merge and Continuous Monitoring**:\n+    \n+    - Once all checks 
pass, the PR is merged, and QitOps logs the quality artifacts for future analysis.\n+\n+## QitOps Workflow\n+\n+```mermaid\n+flowchart TD\n+    A[Developer Opens Pull Request] --> B[QitOps Analyzes PR Context]\n+    B --> C[Risk Analysis Performed]\n+    C --> D[AI Generates Test Cases Based on Risk]\n+    D --> E[Generated Test Cases Added to PR]\n+    E --> F[QA and Dev Review & Refine]\n+    F --> G[Automated Tests Executed in CI/CD]\n+    G --> H{Quality Gates Passed?}\n+    H -->|Yes| I[Merge PR and Log QA Artifacts]\n+    H -->|No| J[Block Merge and Notify Team]\n+```\n+---\n+\n+### **Why QitOps Matters**\n+\n+1. **Accelerates Development**:\n+    \n+    - Embedding QA into Git workflows reduces bottlenecks and handoffs.\n+2. **Improves Quality**:\n+    \n+    - Automated, context-aware QA ensures high coverage and better defect detection.\n+3. **Encourages Collaboration**:\n+    \n+    - QA and development teams work closely through Git, fostering a shared responsibility for quality.\n+4. **Scales with Teams**:\n+    \n+    - QitOps adapts to the scale and complexity of modern software projects, making it suitable for teams of all sizes.\n+\n+---\n+\n+### **Future of QitOps**\n+\n+1. **Enhanced AI Capabilities**:\n+    - Use advanced NLP and diff analysis for deeper insights into changes.\n+2. **Multi-Tool Integration**:\n+    - Seamlessly work with tools like Jira, TestRail, and Slack.\n+3. **Community-Driven Standards**:\n+    - As an open-source initiative, QitOps could establish new standards for QA in DevOps workflows.\n+\n+---\n+\n+### 
Final Thoughts\n+\n+QitOps is not just a workflowâ€”itâ€™s a philosophy that redefines how QA fits into modern software development. \n\\ No newline at end of file\n```\n\nFile: config.yaml\n```diff\nNone\n```\n\nFile: src/config.yaml\n```diff\n@@ -0,0 +1,13 @@\n+providers:\n+  vcs:\n+    github:\n+      token: ${GITHUB_TOKEN}  # Will be loaded from environment\n+  llm:\n+    litellm:\n+      model: "ollama/mistral"\n+      temperature: 0.7\n+  output:\n+    yaml: {}\n+\n+prompt: "prompts/pr_test_case_prompt.txt"\n+output: 
"test_cases_output.yaml"\n\\ No newline at end of file\n```\n\nFile: src/core/factories.py\n```diff\n@@ -32,19 +32,23 @@ def configure(self, config: Dict[str, Any]) -> None:\n             if provider_type in [\'vcs\', \'llm\', \'output\']:\n                 registry = getattr(self, f"{provider_type}_registry")\n                 for name, cfg in provider_config.items():\n-                    # Register provider with its config in one step\n-                    if provider_type == \'vcs\' and name == \'github\':\n-       
                 from services.vcs.github_service import GitHubService\n-                        registry.register(name, GitHubService, cfg)\n-                    elif provider_type == \'llm\' and name == \'litellm\':\n-                        from services.llm.llm_service import LLMService\n-                        registry.register(name, LLMService, cfg)\n-                    elif provider_type == \'output\' and name == \'yaml\':\n-                        from services.output.yaml_writer import YAMLWriter\n-
              registry.register(name, YAMLWriter, cfg)\n-                    elif provider_type == \'output\' and name == \'json\':\n-                        from services.output.json_writer import JSONWriter\n-                        registry.register(name, JSONWriter, cfg)\n+                    # Only register if not already registered\n+        
            if name not in registry.list_providers():\n+                        if provider_type == \'vcs\' and name == \'github\':\n+                            from services.vcs.github_service import GitHubService\n+                            registry.register(name, GitHubService, cfg)\n+                        elif provider_type == \'llm\' and 
name == \'litellm\':\n+                            from services.llm.llm_service import LLMService\n+                            registry.register(name, LLMService, cfg)\n+   
                     elif provider_type == \'output\' and name == \'yaml\':\n+                            from services.output.yaml_writer import YAMLWriter\n+
            registry.register(name, YAMLWriter, cfg)\n+                        elif provider_type == \'output\' and name == \'json\':\n+                            from services.output.json_writer import JSONWriter\n+                            registry.register(name, JSONWriter, cfg)\n+                    else:\n+                        # Update existing provider config\n+                        registry.update_config(name, cfg)\n \n # Global factory manager instance\n factory_manager = FactoryManager()\n\\ No newline 
at end of file\n```\n\nFile: src/core/registry.py\n```diff\n@@ -1,4 +1,4 @@\n-from typing import Dict, Type, Any\n+from typing import Dict, Type, Any, List\n from services.base.vcs_provider import VCSProvider\n from services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n@@ -15,23 +15,31 @@ def register(self, name: str, provider: Type, config: Dict[str, Any] = None) ->\n         if not name or not isinstance(name, str):\n             raise ValueError("Provider name must 
be a non-empty string")\n         \n-        if name in self._providers:\n-            raise ValueError(f"Provider \'{name}\' already registered")\n-            \n-        self._providers[name] = provider\n-        if config:\n-            self._configs[name] = config\n+        if name not in self._providers:\n+            self._providers[name] = provider\n+            if config:\n+                self._configs[name] = config\n+        else:\n+            # Update config if provider exists\n+            self.update_confâ ¹ Generating test cases...r, config: Dict[str, Any]) -> None:\n+        """Update configuration for existing provider"""\n         if name not in self._providers:\n
  raise KeyError(f"Provider \'{name}\' not found")\n-        return self._providers[name]\n-    \n+        if isinstance(config, dict):\n+            self._configs[name] = config\n+        else:\n+            raise ValueError(f"Config must be a dictionary, got {type(config)}")\n+\n+    def get_provider(self, name: str) -> Type:\n+        """Get a provider implementation by name."""\n+        return self._providers.get(name)\n+\n     def get_config(self, name: str) -> Dict[str, Any]:\n         """Get provider configuration by name."""\n         return self._configs.get(name, {})\n-    \n-    def list_providers(self) -> Dict[str, Type]:\n+\n+    def list_providers(self) -> List[str]:\n        
 """List all registered providers."""\n-        return self._providers.copy()\n\\ No newline at end of file\n+        return list(self._providers.keys())\n\\ No newline at end of file\n```\n\nFile: src/core/test_case_generator.py\n```diff\n@@ -1,13 +1,15 @@\n+from datetime import datetime\n from services.base.vcs_provider import VCSProvider\n from 
services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n from models.test_case import TestCase\n from models.pull_request import PullRequest\n-from utils.risk_analysis import analyze_diff\n+from utils.risk_analyzer import RiskAnalyzer\n from rich.console import Console\n from rich.panel import Panel\n+from rich.table import Table\n import yaml\n-from typing import List, Dict\n+from typing import List, Dict, Any\n import re\n import logging\n \n@@ -19,53 +21,101 @@ def __init__(self,\n         self.vcs_provider = vcs_provider\n         self.llm_provider = llm_provider\n         self.output_provider = output_provider\n+        self.risk_analyzer = RiskAnalyzer()\n         self.console = Console()\n+        self.logger = logging.getLogger(__name__)\n \n     def generate(self, repo: str, pr_number: int, output_file: str) -> None:\n         self.console.print(f"[bold blue]ðŸš€ Generating test cases for PR #{pr_number}[/bold blue]")\n         \n+        with self.console.status("[bold yellow]Analyzing PR...") as status:\n+            try:\n+                pr = self.vcs_provider.get_pull_request(repo, pr_number)\n+                risk_analysis = self._analyze_risk(pr)\n+                self._display_risk_analysis(risk_analysis)\n+                \n+                status.update("[bold yellow]Generating test cases...")\n+
prompt = self._load_prompt()\n+                context = self._create_context(pr, risk_analysis)\n+                \n+                llm_output = self.llm_provider.generate(prompt, context)\n+                test_cases = self._parse_test_cases(llm_output)\n+                \n+                if not test_cases:\n+                    self.console.print("[red]Warning: No test cases were generated[/red]")\n+                \n+                self._save_results(pr, risk_analysis, test_cases, output_file)\n+                self.console.print(f"\\n[green]âœ… Results saved to {output_file}[/green]")\n+                \n+            except Exception as e:\n+                self.logger.error(f"Generati
on error: {str(e)}")\n+                self.console.print(f"[red]Error: {str(e)}[/red]")\n+                raise\n+\n+    def _analyze_risk(self, pr: PullRequest) -> Dict[str, Any]:\n+        return self.risk_analyzer.analyze(pr.changes, pr.diffs)\n+\n+    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n         try:\n-   
         pr = self.vcs_provider.get_pull_request(repo, pr_number)\n-            risk_analysis = self._analyze_risk(pr)\n-            \n-            # Load and format prompt\n-            prompt = self._load_prompt()\n-            context = self._create_context(pr, risk_analysis)\n-            \n-            # Generate test cases\n-            self.console.print("[yellow]Generating test cases from LLM...[/yellow]")\n-            llm_output = self.llm_provider.generate(prompt, context)\n-            logging.debug(f"LLM Output: {llm_output}")\n-            \n-            # Parse test cases\n-            test_cases = self._parse_test_cases(llm_output)\n-            if not test_cases:\n-
       self.console.print("[red]Warning: No test cases were generated[/red]")\n+            # Ensure risk_analysis is a dict\n+            if not isinstance(risk_analysis, dict):\n+                self.logger.warning(f"Invalid risk_analysis type: {type(risk_analysis)}")\n+                risk_analysis = {\n+                    "level": "Unknown",\n+                    "factors": [],\n+                    "details": []\n+                }\n             \n-            self._save_results(pr, risk_analysis, test_cases, output_file)\n-            self.console.print(f"\\n[green]âœ… Results saved to {output_file}[/green]")\n+            # Format risk factors safely using \'factors\' instead of \'risk
_factors\'\n+            factors = risk_analysis.get("factors", [])\n+            risk_factors = ", ".join(str(f) for f in factors) if factors else "None"\n             \n+   
         return {\n+                "pr_title": pr.title,\n+                "pr_description": pr.description,\n+                "risk_level": risk_analysis.get("level", "Unknown"),\n+                "risk_factors": risk_factors,\n+                "changes": self._format_changes(pr.changes),\n+                "diffs": self._format_diffs(pr.diffs)\n+            }\n         except Exception as e:\n-            self.console.print(f"[red]Error: {str(e)}[/red]")\n+            self.logger.error(f"Error creating context: {e}")\n             raise\n \n-    def _analyze_risk(self, pr: PullRequest) -> dict:\n-        return {\n-            "level": "High" if len(pr.changes["modified"]) > 5 else "Medium",\n-            "factors": [analyze_diff(diff) for diff in pr.diffs.values()]\n-        }\n+    def _format_changes(self, changes: Dict[str, Any]) -> str:\n+        try:\n+  
          if not isinstance(changes, dict):\n+                return str(changes)\n+            \n+            formatted = []\n+            for change_type, files in changes.items():\n+                if isinstance(files, list):\n+                    formatted.append(f"{change_type.capitalize()}:")\n+                    formatted.extend(f"  - {file}" for file in files)\n+                else:\n+                    formatted.append(f"{change_type}: {files}")\n+            return "\\n".join(formatted)\n+        except Exception as e:\n+            self.logger.error(f"Error formatting changes: {e}")\n+            return str(changes)\n \n-    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n-        return {\n-            "pr": {\n-                "number": pr.number,\n-                "title": pr.title,\n-                "description": pr.description,\n-                "changes": pr.changes,\n-                "diffs": pr.diffs\n-            },\n-            "risk_analysis": risk_analysis\n-        }\n+    def _format_diffs(self, diffs: Dict[str, Any]) -> str:\n+        try:\n+            if not isinstance(diffs, dict):\n+                return str(diffs)\n+                \n+
   formatted = []\n+            for file_path, diff in diffs.items():\n+                formatted.extend([\n+                    f"File: {file_path}",\n+                    "```diff",\n+                    str(diff).strip(),\n+                    "```\\n"\n+                ])\n+            return "\\n".join(formatted)\n+        except Exception as 
e:\n+            self.logger.error(f"Error formatting diffs: {e}")\n+            return str(diffs)\n \n     def _load_prompt(self) -> str:\n         with open(\'prompts/templates/test_case.txt\', \'r\') as f:\n@@ -77,51 +127,84 @@ def _parse_test_cases(self, llm_output: str) -> List[Dict]:\n             # Split into individual test cases\n
    case_blocks = re.split(r\'TC-\\d+:\', llm_output)\n             if not case_blocks[0].strip():\n-                case_blocks = case_blocks[1:]  # Remove empty first split\n+                case_blocks = case_blocks[1:]\n                 \n             for i, block in enumerate(case_blocks, 1):\n                 if not block.strip():\n
           continue\n                     \n-                # Extract fields using multiline patterns\n-                title_match = re.search(r\'Title:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-                priority_match = re.search(r\'Priority:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-                desc_match = re.search(r\'Description:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n+                # Extract fields with improved regex patterns\n+                title_match = re.search(r\'Title:\\s*([^\\n]+)\', block)\n+                priority_match = re.search(r\'Priority:\\s*([^\\n]+)\', block)\n+                description_match = re.search(r\'Description:\\s*([^\\n]+)\', block)\n                 \n                 # Extract steps as list\n                 steps = []\n-                steps_section = re.search(r\'Steps:(.*?)(?=Expected Results:|\\Z)\', block, re.DOTALL)\n-                if steps_section:\n+                steps_match = re.search(r\'Steps:(.*?)(?=Expected Results:|$)\', block, re.DOTALL)\n+    
            if steps_match:\n                     steps = [\n-                        s.strip().lstrip(\'- \') \n-                        for s in steps_section.group(1).strip().split(\'\\n\')\n-                        if s.strip() and s.strip().startswith(\'-\')\n+                        s.strip()[2:] for s in steps_match.group(1).splitlines() \n+                        if s.strip().startswith(\'-\')\n                     ]\n                 \n-                # Extract expected results\n-                expected_match = re.search(r\'Expected Results:\\s*(.*?)(?=\\n\\n|\\Z)\', block, re.DOTALL)\n+                expected_match = re.search(r\'Expected Results:\\s*([^\\n]+(?:\\n(?!\\n).*)*)\', block, re.DOTALL)\n+                \n+                self.logger.debug(f"Title match: {title_match.group(1) if title_match else \'No match\'}")\n                 \n-      
          test_case = {\n+                test_cases.append({\n                     "id": f"TC-{i:03d}",\n                     "title": title_match.group(1).strip() if title_match else "No title",\n                     "priority": priority_match.group(1).strip() if priority_match else "Medium",\n-                    "description": desc_match.group(1).strip() if desc_match else "No description",\n+                    "description": description_match.group(1).strip() if description_match else "No description",\n
           "steps": steps,\n-                    "expected_result": expected_match.group(1).strip() if expected_match else "No expected results"\n-                }\n-        
        test_cases.append(test_case)\n+                    "expected_result": expected_match.group(1).strip() if expected_match else "No expected results",\n+
   "generated_at": datetime.now().isoformat(),\n+                    "approved": False,\n+                    "approved_by": None\n+                })\n                 \n    
     except Exception as e:\n             self.logger.error(f"Error parsing test cases: {str(e)}")\n             self.logger.debug(f"Raw LLM output:\\n{llm_output}")\n-       
 \n+            \n         return test_cases\n \n+    def _extract_field(self, text: str, pattern: str, default: str, flags: re.RegexFlag = 0) -> str:\n+        match = re.search(pattern, text, flags)\n+        return match.group(1).strip() if match else default\n+\n     def _save_results(self, pr: PullRequest, risk_analysis: dict, test_cases: List[Dict], output_file: str) -> None:\n         results = {\n             "pr_number": pr.number,\n             "pr_title": pr.title,\n             "risk_analysis": risk_analysis,\n             "test_cases": test_cases  # Already in dict format\n         }\n-        self.output_provider.write(results, output_file)\n\\ No newline at end of file\n+     
   self.output_provider.write(results, output_file)\n+\n+    def _display_risk_analysis(self, risk_analysis: Dict[str, Any]) -> None:\n+        table = Table(title="Risk Analysis Results")\n+        \n+        table.add_column("Category", style="cyan")\n+        table.add_column("Details", style="magenta")\n+        \n+        # Safely get risk level\n+        level = risk_analysis.get(\'level\', \'Unknown\')\n+        table.add_row("Risk Level", f"[bold]{level}[/bold]")\n+        \n+        # Safely handle factors and 
details\n+        factors = risk_analysis.get(\'factors\', [])\n+        details = risk_analysis.get(\'details\', [])\n+        \n+        # If we have both factors and details, zip them\n+        if factors and details:\n+            for factor, detail in zip(factors, details):\n+                table.add_row(str(factor), str(detail))\n+        # 
Otherwise just show factors\n+        elif factors:\n+            for factor in factors:\n+                table.add_row(str(factor), "")\n+        \n+        self.console.print("\\n")\n+        self.console.print(table)\n+        self.console.print("\\n")\n\\ No newline at end of file\n```\n\nFile: src/main.py\n```diff\n@@ -1,29 +1,47 @@\n from core.factories import factory_manager\n from core.test_case_generator import TestCaseGenerator\n from utils.file_utils import load_config\n+import logging\n+import sys\n+import 
os\n import argparse\n \n def main():\n-    parser = argparse.ArgumentParser(\n-        description=\'Generate test cases from PRs\'\n+    logging.basicConfig(\n+        level=logging.DEBUG,\n+        format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n     )\n-    parser.add_argument(\'repo\')\n-    parser.add_argument(\'pr_number\', 
type=int)\n-    parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n-    parser.add_argument(\'--config\', default=\'config.yaml\')\n-    args = parser.parse_args()\n+    logger = logging.getLogger(__name__)\n \n-    config = load_config(args.config)\n-    factory_manager.configure(config)\n+    try:\n+        config_path = os.path.join(os.path.dirname(__file__), \'config.yaml\')\n+        config = load_config(config_path)\n+        \n+        factory_manager.configure(config)\n \n-    vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n-    llm = factory_manager.llm_factory.create("litellm", \n-
                model=config["providers"]["llm"]["litellm"]["model"], \n-                                             temperature=config["providers"]["llm"]["litellm"]["temperature"])\n-    output = factory_manager.output_factory.create("yaml")\n+        parser = argparse.ArgumentParser(\n+            description=\'Generate test cases from PRs\'\n+        )\n+        parser.add_argument(\'repo\')\n+        parser.add_argument(\'pr_number\', type=int)\n+        parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n+        parser.add_argument(\'--config\', default=\'config.yaml\')\n+        args = parser.parse_args()\n \n-    generator = TestCaseGenerator(vcs, llm, output)\n-    
generator.generate(args.repo, args.pr_number, args.output)\n+        config = load_config(args.config)\n+        factory_manager.configure(config)\n+\n+        vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n+        llm = factory_manager.llm_factory.create("litellm", \n+
                            model=config["providers"]["llm"]["litellm"]["model"], \n+                                                 temperature=config["providers"]["llm"]["litellm"]["temperature"])\n+        output = factory_manager.output_factory.create("yaml")\n+\n+        generator = TestCaseGenerator(vcs, llm, output)\n+        generator.generate(args.repo, args.pr_number, args.output)\n+    except Exception as e:\n+        logger.error(f"Failed to initialize: {str(e)}")\n+        sys.exit(1)\n \n if __name__ == "__main__":\n     main()\n\\ No newline at end of file\n```\n\nFile: src/models/test_case.py\n```diff\n@@ -1,5 +1,6 @@\n from dataclasses import dataclass\n-from typing import 
List\n+from typing import List, Optional\n+from datetime import datetime\n \n @dataclass\n class TestCase:\n@@ -8,4 +9,8 @@ class TestCase:\n     priority: str\n     description: str\n     steps: List[str]\n-    expected_result: str\n\\ No newline at end of file\n+    expected_result: str\n+    generated_at: datetime = datetime.now()\n+    approved: bool = False\n+    approved_by: Optional[str] = None\n+    risk_factors: List[str] = None\n\\ No newline at end of file\n```\n\nFile: src/pr_test_cases.yaml\n```diff\n@@ -1,53 +1,71 @@\n pr_number: 1\n pr_title: Bump starlette from 0.27.0 to 0.40.0 in /src\n risk_analysis:\n-  level: Medium\n+  level: High\n   factors:\n-  - []\n+  - Security Risk\n+  - Dependency Changes\n+  details:\n+  - Security-sensitive code changes detected\n+  - Package dependencies modified\n test_cases:\n - id: TC-001\n-  title: No title\n- 
 priority: Medium\n-  description: No description\n+  title: Verify the security of the updated Starlette dependency in the application\n+  priority: High\n+  description: Check if any newly introduced vulnerabilities or security risks are\n+    present in the updated version (0.40.0) of Starlette, which could potentially\n+    impact the overall security of the application.\n   steps:\n-  - Import starlette module in a test script\n-  - Write a test function which was working with the previous version and check if\n-  
  it works as expected with the new version\n+  - Install the updated version of Starlette (0.40.0) in a separate development environment\n+  - Run a comprehensive vulnerability scan on the isolated environment using tools\n+    like OWASP ZAP or Bandit\n+  - Check for any reported vulnerabilities, security issues, or warnings related to\n+    the 
updated Starlette package\n   - \'\'\n-  expected_result: The test function should pass without any errors or unexpected\n-    behaviors.\n+  expected_result: The security scan should not reveal any critical or high-severity\n+    vulnerabilities, ensuring that the application remains secure. If any issues are\n+    found, they must be addressed and fixed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.979267\'\n+  approved: false\n+  approved_by: null\n - id: TC-002\n-  title: No title\n+  title: Ensure compatibility of the updated Starlette dependency with other dependencies\n+    in the application\n   priority: Medium\n-  description: No description\n+  description: Validate that the updated version (0.40.0) of Starlette is compatible\n+    with the existing versions of other dependencies within the application, to prevent\n+    any potential conflicts or errors during runtime.\n   steps:\n-  - Install and run OWASP ZAP or bandit on the new starlette codebase\n-  - Analyze the results for any potential security vulnerabilities\n+  - Create a development environment containing all the dependencies listed in the\n+    requirements.txt file (excluding Starlette for now)\n+  - Install the updated version of Starlette (0.40.0) in this environment\n+  - Run the application to check if there are any errors, warnings, or unexpected\n+    behavior caused by the updated Starlette dependency\n   - \'\'\n-  expected_result: The test should not find any critical or high severity security\n-    vulnerabilities in the new version.\n+  expected_result: The application should run smoothly without any critical errors\n+    or warning messages related to the updated Starlette package, demonstrating compatibility\n+ 
   with other dependencies. If any issues occur, they must be resolved before merging\n+    this update.\n+  generated_at: \'2025-01-28T14:27:14.979975\'\n+  approved: false\n+  approved_by: null\n - id: TC-003\n-  title: No title\n-  priority: Medium\n-  description: No description\n-  steps:\n-  - Create a custom exception class\n-  - Use the custom exception in a route handler and trigger it intentionally\n-  - Check if the exception is handled correctly by starlette and an appropriate error\n-    message is returned to the client\n-  - \'\'\n-  expected_result: Starlette should be able to handle the custom exception gracefully,\n-    returning an appropriate error message to the client.\n-- id: TC-004\n-  title: No title\n+  title: Test error handling of the updated Starlette dependency in the application\n   priority: Medium\n-  description: No description\n+  description: Verify that the updated version (0.40.0) of Starlette handles errors\n+    and exceptions gracefully, ensuring a good user experience even when unexpected\n+  
  situations occur.\n   steps:\n-  - Create a test script which uses an API or functionality from starlette 0.27.0\n-  - Run the test script with both versions (0.27.0 and 0.40.0) of starlette installed\n-    separately\n-  - Check if there are any differences in behavior or outcomes between the two versions\n+  - Create test scenarios where exceptional conditions are intentionally triggered\n+    within the application (e.g., by providing invalid inputs or forcing certain edge\n+    cases)\n+  - Observe how the updated Starlette dependency handles these exceptions and errors,\n+    checking for appropriate logging, error messages, and recovery mechanisms\n   - \'\'\n-  expected_result: There should not be any noticeable differences in behavior or outcomes\n-    when using the same API or functionality with both versions (0.27.0 and 0.40.0)\n-    of starlette.\n+  expected_result: The updated Starlette dependency should handle errors effectively,\n+    providing descriptive error messages to help troubleshoot issues and maintaining\n+    application stability in the event of exceptional conditions. If any issues occur,\n+    they must be addressed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.980992\'\n+  approved: false\n+  approved_by: null\n```\n\nFile: src/prompts/pr_test_case_prompt.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/prompts/templates/test_case.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: 
[Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- 
Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/services/llm/llm_service.py\n```diff\n@@ -27,29 +27,21 @@ def generate(self, prompt: str, context: Dict[str, Any]) -> str:\n         return result\n \n     def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:\n-        pr = context.get(\'pr\', {})\n-        risk = 
context.get(\'risk_analysis\', {})\n-        \n-        # Format changes\n-        changes_str = "\\n".join(\n-            f"{k}: {\', \'.join(v)}" \n-            for k, v in 
pr.get(\'changes\', {}).items() \n-            if v\n-        )\n-        \n-        # Format diffs\n-        diffs_str = "\\n".join(\n-            f"File: {fname}\\n{diff}" \n-            for fname, diff in pr.get(\'diffs\', {}).items()\n-        )\n-        \n-        return prompt.format(\n-            pr_title=pr.get(\'title\', \'\'),\n-       
     pr_description=pr.get(\'description\', \'\'),\n-            risk_level=risk.get(\'level\', \'\'),\n-            changes=changes_str,\n-            diffs=diffs_str\n-     
   )\n+        try:\n+            return prompt.format(\n+                pr_title=context.get(\'pr_title\', \'\'),\n+                pr_description=context.get(\'pr_description\', \'\'),\n+                risk_level=context.get(\'risk_level\', \'\'),\n+                risk_factors=context.get(\'risk_factors\', \'\'),\n+                changes=context.get(\'changes\', \'\'),\n+                diffs=context.get(\'diffs\', \'\')\n+            )\n+        except KeyError as e:\n+            self.logger.error(f"Missing required key in context: {e}")\n+            raise\n+        except Exception as e:\n+            self.logger.error(f"Error formatting prompt: {e}")\n+            raise\n \n     def _format_changes(self, changes: Dict[str, Any]) -> str:\n         result = []\n```\n\nFile: src/utils/file_utils.py\n```diff\n@@ -1,5 +1,40 @@\n+import os\n import yaml\n+import logging\n+from typing import Dict, Any\n+import re\n \n-def load_config(path: str) -> dict:\n-    with open(path, \'r\', encoding=\'utf-8\') as f:\n-        return yaml.safe_load(f)\n\\ No newline at end of file\n+def load_config(path: str) -> Dict[str, Any]:\n+    logger = logging.getLogger(__name__)\n+    \n+    if not os.path.exists(path):\n+        logger.error(f"Config file not found: {path}")\n+        raise FileNotFoundError(f"Config file not found: {path}")\n+        \n+    with open(path, \'r\') as f:\n+  
      config = yaml.safe_load(f)\n+        \n+    if not config:\n+        raise ValueError("Empty config file")\n+        \n+    # Replace environment variables\n+    config 
= _replace_env_vars(config)\n+    \n+    return config\n+\n+def _replace_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:\n+    """Recursively replace ${VAR} with environment variable values"""\n+    if isinstance(config, dict):\n+        return {k: _replace_env_vars(v) for k, v in config.items()}\n+    elif isinstance(config, list):\n+        return [_replace_env_vars(v) for v in config]\n+    elif isinstance(config, str):\n+        pattern = r\'\\${([^}]+)}\'\n+        match = re.search(pattern, config)\n+        if match:\n+            env_var = match.group(1)\n+            env_value = os.getenv(env_var)\n+            if not env_value:\n+                raise ValueError(f"Environment variable {env_var} not set")\n+            return config.replace(match.group(0), env_value)\n+    return config\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analysis.py\n```diff\n@@ -1,24 +1,45 @@\n from enum import Enum\n+from typing import Dict, List, Any\n+import re\n \n class RiskLevel(Enum):\n     LOW = "Low"\n     MEDIUM = "Medium"\n     HIGH = "High"\n \n-def analyze_diff(diff_content):\n-    """Analyze diff content for risk patterns."""\n-    risk_patterns = {\n-        "sql": "Database changes",\n-        "await": "Async flow changes",\n-        "try": "Error handling changes",\n-        "import": "Dependency changes",\n-        "class": "Class structure changes",\n-   
     "function": "Function signature changes"\n-    }\n+class RiskFactor(Enum):\n+    DEPENDENCY = "Dependency Changes"\n+    SECURITY = "Security Impact"\n+    BREAKING = "Breaking Changes"\n+    COVERAGE = "Test Coverage"\n \n-    found_patterns = []\n-    for pattern, risk in risk_patterns.items():\n-        if pattern in diff_content.lower():\n-            found_patterns.append(risk)\n-\n-    return found_patterns\n\\ No newline at end of file\n+def analyze_risk(changes: Dict[str, List[str]], diffs: Dict[str, str]) 
-> Dict[str, Any]:\n+    risk_factors = []\n+    \n+    # Check dependency changes\n+    if any(\'requirements.txt\' in file or \'package.json\' in file for file in changes.get(\'modified\', [])):\n+        risk_factors.append(RiskFactor.DEPENDENCY.value)\n+    \n+    # Check security patterns\n+    security_patterns = [\n+        r\'auth[orization]*\',\n+        r\'password\',\n+        r\'secret\',\n+        r\'token\',\n+        r\'crypt\'\n+    ]\n+    \n+    if any(re.search(\'|\'.join(security_patterns), diff, re.I) for diff in diffs.values()):\n+        risk_factors.append(RiskFactor.SECURITY.value)\n+    \n+    # Determine risk level\n+    risk_level = RiskLevel.LOW\n+    if len(risk_factors) >= 2:\n+        risk_level = RiskLevel.HIGH\n+    elif len(risk_factors) == 1:\n+        risk_level = RiskLevel.MEDIUM\n+    \n+    return {\n+        "level": risk_level.value,\n+        "factors": risk_factors\n+    }\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analyzer.py\n```diff\n@@ -0,0 +1,85 @@\n+from typing import Dict, List, Any, Union\n+from enum import Enum\n+import re\n+import logging\n+\n+class RiskLevel(Enum):\n+    LOW = "Low"\n+    MEDIUM = "Medium"\n+    HIGH = "High"\n+\n+class RiskAnalyzer:\n+    def __init__(self):\n+        self.logger = logging.getLogger(__name__)\n+        self.security_patterns = [\n+            r\'auth\\w*\',\n+            r\'password\',\n+            r\'secret\',\n+            r\'token\',\n+            r\'crypt\'\n+        ]\n+        self.breaking_patterns = [\n+            r\'break.*change\',\n+            r\'deprecat\\w*\',\n+            r\'remov\\w+\\s+\\w+\',\n+            r\'delet\\w+\\s+\\w+\'\n+        ]\n+\n+    def analyze(self, changes: Union[Dict[str, List[str]], str], diffs: Union[Dict[str, str], str]) -> Dict[str, Any]:\n+        try:\n+            # Normalize inputs\n+            changes_dict = changes if isinstance(changes, dict) else {"modified": [str(changes)]}\n+            diffs_dict = diffs if isinstance(diffs, dict) else {"file": str(diffs)}\n+            \n+            risk_factors = []\n+            details = []\n+\n+            # Check security risks\n+            if self._check_security_risks(diffs_dict):\n+                risk_factors.append("Security Risk")\n+                details.append("Security-sensitive code changes detected")\n+\n+            # Check dependency changes\n+            if self._check_dependency_changes(changes_dict):\n+                risk_factors.append("Dependency Changes")\n+                details.append("Package dependencies modified")\n+\n+            # Check breaking changes\n+            if self._check_breaking_changes(diffs_dict):\n+                risk_factors.append("Breaking Changes")\n+                details.append("Breaking changes detected")\n+\n+            level = self._determine_risk_level(risk_factors)\n+            \n+            return {\n+                "level": level,\n+                "factors": risk_factors,\n+                "details": details\n+            }\n+        except Exception as e:\n+            self.logger.error(f"Error in risk analysis: {e}")\n+        
    return {\n+                "level": "High",\n+                "factors": ["Analysis Error"],\n+                "details": [str(e)]\n+            }\n+\n+    def _check_security_risks(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.security_patterns)\n+
 for content in diffs.values()\n+        )\n+\n+    def _check_dependency_changes(self, changes: Dict[str, List[str]]) -> bool:\n+        dependency_files = [\'requirements.txt\', \'package.json\', \'build.gradle\', \'pom.xml\']\n+        modified = changes.get(\'modified\', [])\n+        return any(dep in str(file) for dep in dependency_files for 
file in modified)\n+\n+    def _check_breaking_changes(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.breaking_patterns)\n+            for content in diffs.values()\n+        )\n+\n+    def _determine_risk_level(self, factors: List[str]) -> str:\n+        return "High" if len(factors) >= 2 else "Medium" if factors else "Low"\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_patterns.py\n```diff\n@@ -0,0 +1,22 @@\n+from enum import Enum\n+from typing import List, Pattern\n+import re\n+\n+class RiskPatternType(Enum):\n+    SECURITY = "security"\n+    BREAKING = "breaking"\n+    PERFORMANCE = "performance"\n+    COMPLEXITY = "complexity"\n+\n+class RiskPattern:\n+    def __init__(self, pattern: str, risk_type: RiskPatternType, weight: int = 1):\n+        self.pattern = re.compile(pattern, re.IGNORECASE)\n+        self.type = risk_type\n+        self.weight = weight\n+\n+RISK_PATTERNS = [\n+    RiskPattern(r\'auth|login|password|secret|token\', RiskPatternType.SECURITY, 3),\n+    RiskPattern(r\'break.*change|deprecat|remove[d]?\\s+\\w+\', RiskPatternType.BREAKING, 2),\n+    RiskPattern(r\'performance|optimize|slow|fast\', RiskPatternType.PERFORMANCE, 2),\n+    RiskPattern(r\'complex|complicated|confusing\', RiskPatternType.COMPLEXITY, 1)\n+]\n\\ No newline at end of file\n```\n\n\nGenerate specific test cases addressing the identified risk factors.\nFocus on security, compatibility, and error handling.\n\nFormat each test case exactly as follows:\n\nTC-001:\n- 
Title: [Specific test objective]\n- Priority: [Based on risk level]\n- Description: [Detailed scenario]\n- Steps:\n  - [Clear, actionable step]\n  - [Expected interaction]\n- 
Expected Results: [Verifiable outcome]\n\nGenerate at least 3 test cases.\n\n', 'options': {'temperature': 0.7}, 'stream': False}'


2025-01-28 14:50:17,103 - LiteLLM - DEBUG - 

POST Request Sent from LiteLLM:
curl -X POST \
http://localhost:11434/api/generate \
-d '{'model': 'mistral', 'prompt': '### User:\nGiven the following pull request changes:\n\nTitle: Feat risk analysis\nDescription: \n\nRisk Assessment:\n- Level: High\n- Factors: - Analysis Error: expected string or bytes-like object, got \'NoneType\'\n\nChanged Files:\nAdded:\n  - ABOUT.md\n  - config.yaml\n  - src/config.yaml\n  - src/utils/risk_analyzer.py\n  - src/utils/risk_patterns.py\nModified:\n  - src/core/factories.py\n  - src/core/registry.py\n  - src/core/test_case_generator.py\n  - src/main.py\n  - src/models/test_case.py\n  - src/pr_test_cases.yaml\n  - src/prompts/pr_test_case_prompt.txt\n  - src/prompts/templates/test_case.txt\n  - src/services/llm/llm_service.py\n  - src/utils/file_utils.py\n  - src/utils/risk_analysis.py\nRemoved:\n\nDiffs:\nFile: ABOUT.md\n```diff\n@@ -0,0 +1,133 @@\n+### **QitOps: Concept and Vision**\n+\n+**QitOps** is the practice of embedding quality assurance (QA) processes directly into the Git-based workflows of modern software development. It ensures that QA is not a siloed or afterthought 
activity but a continuous, collaborative, and automated part of the development lifecycle.\n+\n+At its core, QitOps leverages [GitOps](https://shalb.com/blog/gitops-an-introduction-to-gitops-principles-and-practices/) principlesâ€”treating everything as codeâ€”to integrate testing, risk analysis, and quality metrics into the workflows developers and QA engineers already use.\n+\n+---\n+\n+### **Core Tenets of QitOps**\n+\n+1. **QA as Code**:\n+    \n+    - Test cases, risk assessments, and quality gates are stored and managed as code in [Git repositories](https://www.simplilearn.com/tutorials/git-tutorial/what-is-a-git-repository).\n+    - [Test artifacts](https://artoftesting.com/test-artifacts-deliverables) (e.g., test cases, coverage reports) are versioned alongside application code.\n+2. **Seamless Integration**:\n+    \n+    - QA workflows are triggered automatically by Git events (e.g., [pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests), commits, merges).\n+    - QA becomes a natural part of the developer workflow, reducing handoffs and friction.\n+3. **Context-Aware Quality Assurance**:\n+    \n+    - QA processes adapt dynamically based on the context of code changes (e.g., [diffs](https://www.atlassian.com/git/tutorials/saving-changes/git-diff#:~:text=Diffing%20is%20a%20function%20that,%2C%20branches%2C%20files%20and%20more.), file types, PR descriptions).\n+    - Focused and relevant testing is prioritized, improving efficiency.\n+4. **Collaboration-Driven**:\n+    \n+    - QA is integrated into pull requests and [code reviews](https://about.gitlab.com/topics/version-control/what-is-code-review/), fostering collaboration between developers and QA engineers.\n+    - Teams can comment, refine, and approve test cases directly within PRs.\n+5. **Automation First**:\n+    \n+    - Automated pipelines handle test case generation, execution, and validation.\n+    - [Risk analysis](https://www.lambdatest.com/blog/how-to-incorporate-risk-management-strategy-in-testing/) and quality metrics are generated programmatically.\n+6. **Transparency and Metrics**:\n+    \n+    - QA workflows provide clear visibility into [quality metrics](https://www.testrail.com/qa-metrics/#glossary-of-qa-metric-formulas-5), such as test coverage, defect rates, and risk levels.\n+    - Metrics are made accessible to all stakeholders, enhancing accountability.\n+\n+---\n+\n+### **Key Components of QitOps**\n+\n+1. **GitHub Integration**:\n+    \n+    - QA workflows are tightly integrated with GitHub (or similar platforms, such as Bitbucket, Gitlab, CodeCommit etc) using APIs and GitHub Actions.\n+    - Test case generation and validation pipelines are triggered by PR events.\n+2. **AI-Powered Test Case Generation**:\n+    \n+    - Local or cloud-based LLMs analyze PRs, diffs, and descriptions to generate contextually relevant test cases.\n+  
  - AI assists with edge case identification, regression risks, and performance considerations.\n+3. **Dynamic Risk Assessment**:\n+    \n+    - Automated analysis of code changes to identify high-risk areas.\n+    - Risk levels influence the prioritization of test cases and QA workflows.\n+4. **Continuous Quality Feedback**:\n+    \n+    - PR comments, dashboards, and notifications provide real-time feedback on quality status.\n+    - Quality gates ensure critical issues are addressed before merging.\n+5. **Quality Pipelines**:\n+    \n+    - GitHub Actions or CI/CD tools execute automated tests based on generated cases.\n+    - Pipelines validate QA artifacts for completeness and consistency.\n+\n+---\n+\n+### **QitOps in Action**\n+\n+#### Example Workflow:\n+\n+1. **Developer Opens a PR**:\n+    \n+    - QitOps triggers an automated analysis of the PR (e.g., 
files changed, description, diff).\n+2. **Automated Test Case Generation**:\n+    \n+    - AI generates test cases based on the context of the changes.\n+    - Generated test 
cases are added as a comment or file in the PR.\n+3. **Risk Assessment and Metrics**:\n+    \n+    - The system evaluates the risk level of the changes and suggests areas requiring manual QA.\n+4. **Team Collaboration**:\n+    \n+    - QA engineers and developers review and refine test cases within the PR.\n+5. **Automated Validation**:\n+    \n+  
  - Pipelines execute tests and validate QA metrics, blocking the merge if quality gates are not met.\n+6. **Merge and Continuous Monitoring**:\n+    \n+    - Once all checks 
pass, the PR is merged, and QitOps logs the quality artifacts for future analysis.\n+\n+## QitOps Workflow\n+\n+```mermaid\n+flowchart TD\n+    A[Developer Opens Pull Request] --> B[QitOps Analyzes PR Context]\n+    B --> C[Risk Analysis Performed]\n+    C --> D[AI Generates Test Cases Based on Risk]\n+    D --> E[Generated Test Cases Added to PR]\n+    E --> F[QA and Dev Review & Refine]\n+    F --> G[Automated Tests Executed in CI/CD]\n+    G --> H{Quality Gates Passed?}\n+    H -->|Yes| I[Merge PR and Log QA Artifacts]\n+    H -->|No| J[Block Merge and Notify Team]\n+```\n+---\n+\n+### **Why QitOps Matters**\n+\n+1. **Accelerates Development**:\n+    \n+    - Embedding QA into Git workflows reduces bottlenecks and handoffs.\n+2. **Improves Quality**:\n+    \n+    - Automated, context-aware QA ensures high coverage and better defect detection.\n+3. **Encourages Collaboration**:\n+    \n+    - QA and development teams work closely through Git, fostering a shared responsibility for quality.\n+4. **Scales with Teams**:\n+    \n+    - QitOps adapts to the scale and complexity of modern software projects, making it suitable for teams of all sizes.\n+\n+---\n+\n+### **Future of QitOps**\n+\n+1. **Enhanced AI Capabilities**:\n+    - Use advanced NLP and diff analysis for deeper insights into changes.\n+2. **Multi-Tool Integration**:\n+    - Seamlessly work with tools like Jira, TestRail, and Slack.\n+3. **Community-Driven Standards**:\n+    - As an open-source initiative, QitOps could establish new standards for QA in DevOps workflows.\n+\n+---\n+\n+### 
Final Thoughts\n+\n+QitOps is not just a workflowâ€”itâ€™s a philosophy that redefines how QA fits into modern software development. \n\\ No newline at end of file\n```\n\nFile: config.yaml\n```diff\nNone\n```\n\nFile: src/config.yaml\n```diff\n@@ -0,0 +1,13 @@\n+providers:\n+  vcs:\n+    github:\n+      token: ${GITHUB_TOKEN}  # Will be loaded from environment\n+  llm:\n+    litellm:\n+      model: "ollama/mistral"\n+      temperature: 0.7\n+  output:\n+    yaml: {}\n+\n+prompt: "prompts/pr_test_case_prompt.txt"\n+output: 
"test_cases_output.yaml"\n\\ No newline at end of file\n```\n\nFile: src/core/factories.py\n```diff\n@@ -32,19 +32,23 @@ def configure(self, config: Dict[str, Any]) -> None:\n             if provider_type in [\'vcs\', \'llm\', \'output\']:\n                 registry = getattr(self, f"{provider_type}_registry")\n                 for name, cfg in provider_config.items():\n-                    # Register provider with its config in one step\n-                    if provider_type == \'vcs\' and name == \'github\':\n-       
                 from services.vcs.github_service import GitHubService\n-                        registry.register(name, GitHubService, cfg)\n-                    elif provider_type == \'llm\' and name == \'litellm\':\n-                        from services.llm.llm_service import LLMService\n-                        registry.register(name, LLMService, cfg)\n-                    elif provider_type == \'output\' and name == \'yaml\':\n-                        from services.output.yaml_writer import YAMLWriter\n-
              registry.register(name, YAMLWriter, cfg)\n-                    elif provider_type == \'output\' and name == \'json\':\n-                        from services.output.json_writer import JSONWriter\n-                        registry.register(name, JSONWriter, cfg)\n+                    # Only register if not already registered\n+        
            if name not in registry.list_providers():\n+                        if provider_type == \'vcs\' and name == \'github\':\n+                            from services.vcs.github_service import GitHubService\n+                            registry.register(name, GitHubService, cfg)\n+                        elif provider_type == \'llm\' and 
name == \'litellm\':\n+                            from services.llm.llm_service import LLMService\n+                            registry.register(name, LLMService, cfg)\n+   
                     elif provider_type == \'output\' and name == \'yaml\':\n+                            from services.output.yaml_writer import YAMLWriter\n+
            registry.register(name, YAMLWriter, cfg)\n+                        elif provider_type == \'output\' and name == \'json\':\n+                            from services.output.json_writer import JSONWriter\n+                            registry.register(name, JSONWriter, cfg)\n+                    else:\n+                        # Update existing provider config\n+                        registry.update_config(name, cfg)\n \n # Global factory manager instance\n factory_manager = FactoryManager()\n\\ No newline 
at end of file\n```\n\nFile: src/core/registry.py\n```diff\n@@ -1,4 +1,4 @@\n-from typing import Dict, Type, Any\n+from typing import Dict, Type, Any, List\n from services.base.vcs_provider import VCSProvider\n from services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n@@ -15,23 +15,31 @@ def register(self, name: str, provider: Type, config: Dict[str, Any] = None) ->\n         if not name or not isinstance(name, str):\n             raise ValueError("Provider name must 
be a non-empty string")\n         \n-        if name in self._providers:\n-            raise ValueError(f"Provider \'{name}\' already registered")\n-            \n-        self._providers[name] = provider\n-        if config:\n-            self._configs[name] = config\n+        if name not in self._providers:\n+            self._providers[name] = provider\n+            if config:\n+                self._configs[name] = config\n+        else:\n+            # Update config if provider exists\n+            self.update_config(name, config)\n     \n-    def get_provider(self, name: str) -> Type:\n-        """Get a provider implementation by name."""\n+    def update_config(self, name: str, config: Dict[str, Any]) -> None:\n+        """Update configuration for existing provider"""\n         if name not in self._providers:\n             raise KeyError(f"Provider \'{name}\' not found")\n-        return self._providers[name]\n-    \n+        if isinstance(config, dict):\n+            self._configs[name] = config\n+        else:\n+            raise ValueError(f"Config must be a dictionary, got {type(config)}")\n+\n+    def get_provider(self, name: str) -> Type:\n+        """Get a provider implementation by name."""\n+        return self._providers.get(name)\n+\n     def get_config(self, name: str) -> Dict[str, Any]:\n         """Get provider configuration by name."""\n         return self._configs.get(name, {})\n-    \n-    def list_providers(self) -> Dict[str, Type]:\n+\n+    def list_providers(self) -> List[str]:\n         """List all registered providers."""\n-        return self._providers.copy()\n\\ No newline at end of file\n+        return list(self._providers.keys())\n\\ No newline at end of file\n```\n\nFile: src/core/test_case_generator.py\n```diff\n@@ -1,13 +1,15 @@\n+from datetime import datetime\n from services.base.vcs_provider import VCSProvider\n from services.base.llm_provider import LLMProvider\n from services.base.output_provider import OutputProvider\n from models.test_case import TestCase\n from models.pull_request import PullRequest\n-from utils.risk_analysis import analyze_diff\n+from utils.risk_analyzer import RiskAnalyzer\n from rich.console import Console\n from rich.panel import Panel\n+from rich.table import Table\n import yaml\n-from typing import List, Dict\n+from typing import List, Dict, Any\n import re\n import logging\n \n@@ -19,53 +21,101 @@ def __init__(self,\n         self.vcs_provider = vcs_provider\n         self.llm_provider = llm_provider\n         self.output_provider = output_provider\n+        self.risk_analyzer = RiskAnalyzer()\n         self.console = Console()\n+        self.logger = logging.getLogger(__name__)\n \n     def generate(self, repo: str, pr_number: int, output_file: str) -> None:\n         self.console.print(f"[bold blue]ðŸš€ Generating test cases for PR #{pr_number}[/bold blue]")\n         \n+        with self.console.status("[bold yellow]Analyzing PR...") as status:\n+     
       try:\n+                pr = self.vcs_provider.get_pull_request(repo, pr_number)\n+                risk_analysis = self._analyze_risk(pr)\n+                self._display_risk_analysis(risk_analysis)\n+                \n+                status.update("[bold yellow]Generating test cases...")\n+                prompt = self._load_prompt()\n+    
            context = self._create_context(pr, risk_analysis)\n+                \n+                llm_output = self.llm_provider.generate(prompt, context)\n+                test_cases = self._parse_test_cases(llm_output)\n+                \n+                if not test_cases:\n+                    self.console.print("[red]Warning: No test cases were generated[/red]")\n+                \n+                self._save_results(pr, risk_analysis, test_cases, output_file)\n+                self.console.print(f"\\n[green]âœ… Res
ults saved to {output_file}[/green]")\n+                \n+            except Exception as e:\n+                self.logger.error(f"Generation error: {str(e)}")\n+
    self.console.print(f"[red]Error: {str(e)}[/red]")\n+                raise\n+\n+    def _analyze_risk(self, pr: PullRequest) -> Dict[str, Any]:\n+        return self.risk_analyzer.analyze(pr.changes, pr.diffs)\n+\n+    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n         try:\n-            pr = self.vcs_provider.get_pull_request(repo, pr_number)\n-            risk_analysis = self._analyze_risk(pr)\n-            \n-            # Load and format prompt\n-            prompt = self._load_prompt()\n-            context = self._create_context(pr, risk_analysis)\n-            \n-            # Generate test cases\n-            self.console.print("[yellow]Generating test cases from LLM...[/yellow]")\n-            llm_output = self.llm_provider.generate(prompt, context)\n-            logging.debug(f"LLM Output: {llm_output}")\n-
\n-            # Parse test cases\n-            test_cases = self._parse_test_cases(llm_output)\n-            if not test_cases:\n-                self.console.print("[red]Warning: No test cases were generated[/red]")\n+            # Ensure risk_analysis is a dict\n+            if not isinstance(risk_analysis, dict):\n+                self.logger.warning(f"Invalid risk_analysis type: {type(risk_analysis)}")\n+                risk_analysis = {\n+                    "level": "Unknown",\n+                    "factors": [],\n+                    "details": []\n+                }\n             \n-            self._save_results(pr, risk_analysis, test_cases, output_file)\n-            self.console.print(f"\\n[green]âœ… Results saved to {output_file}[/green]")\n+            # Format risk factors safely using \'factors\' instead of \'risk_factors\'\n+            factors = 
risk_analysis.get("factors", [])\n+            risk_factors = ", ".join(str(f) for f in factors) if factors else "None"\n             \n+            return {\n+
 "pr_title": pr.title,\n+                "pr_description": pr.description,\n+                "risk_level": risk_analysis.get("level", "Unknown"),\n+                "risk_factors": risk_factors,\n+                "changes": self._format_changes(pr.changes),\n+                "diffs": self._format_diffs(pr.diffs)\n+            }\n         except Exception as e:\n-            self.console.print(f"[red]Error: {str(e)}[/red]")\n+            self.logger.error(f"Error creating context: {e}")\n             raise\n \n-    def _analyze_risk(self, pr: PullRequest) -> dict:\n-        return {\n-            "level": "High" if len(pr.changes["modified"]) > 5 else "Medium",\n-            "factors": [analyze_diff(diff) for diff in pr.diffs.values()]\n-        }\n+    def _format_changes(self, changes: Dict[str, Any]) -> str:\n+        try:\n+            if not isinstance(changes, dict):\n+                return str(changes)\n+            \n+            formatted = []\n+            for change_type, files in changes.items():\n+                if isinstance(files, list):\n+                    formatted.append(f"{change_type.capitalize()}:")\n+                    formatted.extend(f"  - {file}" for file in files)\n+
     else:\n+                    formatted.append(f"{change_type}: {files}")\n+            return "\\n".join(formatted)\n+        except Exception as e:\n+            self.logger.error(f"Error formatting changes: {e}")\n+            return str(changes)\n \n-    def _create_context(self, pr: PullRequest, risk_analysis: dict) -> dict:\n-        return {\n-            "pr": {\n-                "number": pr.number,\n-                "title": pr.title,\n-                "description": pr.description,\n-                "changes": pr.changes,\n-                "diffs": pr.diffs\n-            },\n-            "risk_analysis": risk_analysis\n-        }\n+    def _format_diffs(self, diffs: Dict[str, Any]) -> str:\n+        try:\n+            if not isinstance(diffs, dict):\n+                return str(diffs)\n+                \n+            formatted = []\n+            for file_path, diff in diffs.items():\n+                formatted.extend([\n+                    f"File: {file_path}",\n+                    "```diff",\n+                    str(diff).strip(),\n+                    "```\\n"\n+                ])\n+            return "\\n".join(formatted)\n+        except Exception as e:\n+            self.logger.error(f"Error formatting diffs: {e}")\n+            return str(diffs)\n \n     def _load_prompt(self) -> str:\n         with open(\'prompts/templates/test_case.txt\', \'r\') as f:\n@@ -77,51 +127,84 @@ def _parse_test_cases(self, llm_output: str) -> List[Dict]:\n             # Split into individual test cases\n             case_blocks = re.split(r\'TC-\\d+:\', llm_output)\n             if not case_blocks[0].strip():\n-                case_blocks = case_blocks[1:]  # Remove empty first split\n+                case_blocks = case_blocks[1:]\n                 \n             for i, block in enumerate(case_blocks, 1):\n                 if not block.strip():\n                     continue\n
       \n-                # Extract fields using multiline patterns\n-                title_match = re.search(r\'Title:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-        
        priority_match = re.search(r\'Priority:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n-                desc_match = re.search(r\'Description:\\s*(.*?)(?=\\n-|\\Z)\', block, re.MULTILINE)\n+                # Extract fields with improved regex patterns\n+                title_match = re.search(r\'Title:\\s*([^\\n]+)\', block)\n+
  priority_match = re.search(r\'Priority:\\s*([^\\n]+)\', block)\n+                description_match = re.search(r\'Description:\\s*([^\\n]+)\', block)\n                 \n   
              # Extract steps as list\n                 steps = []\n-                steps_section = re.search(r\'Steps:(.*?)(?=Expected Results:|\\Z)\', block, re.DOTALL)\n- 
               if steps_section:\n+                steps_match = re.search(r\'Steps:(.*?)(?=Expected Results:|$)\', block, re.DOTALL)\n+                if steps_match:\n      
               steps = [\n-                        s.strip().lstrip(\'- \') \n-                        for s in steps_section.group(1).strip().split(\'\\n\')\n-
         if s.strip() and s.strip().startswith(\'-\')\n+                        s.strip()[2:] for s in steps_match.group(1).splitlines() \n+                        if s.strip().startswith(\'-\')\n                     ]\n                 \n-                # Extract expected results\n-                expected_match = re.search(r\'Expected Results:\\s*(.*?)(?=\\n\\n|\\Z)\', block, re.DOTALL)\n+                expected_match = re.search(r\'Expected Results:\\s*([^\\n]+(?:\\n(?!\\n).*)*)\', block, re.DOTALL)\n+
   \n+                self.logger.debug(f"Title match: {title_match.group(1) if title_match else \'No match\'}")\n                 \n-                test_case = {\n+
       test_cases.append({\n                     "id": f"TC-{i:03d}",\n                     "title": title_match.group(1).strip() if title_match else "No title",\n
         "priority": priority_match.group(1).strip() if priority_match else "Medium",\n-                    "description": desc_match.group(1).strip() if desc_match else "No description",\n+                    "description": description_match.group(1).strip() if description_match else "No description",\n                     "steps": steps,\n-      
              "expected_result": expected_match.group(1).strip() if expected_match else "No expected results"\n-                }\n-                test_cases.append(test_case)\n+                    "expected_result": expected_match.group(1).strip() if expected_match else "No expected results",\n+                    "generated_at": datetime.now().isoformat(),\n+                    "approved": False,\n+                    "approved_by": None\n+                })\n                 \n         except Exception as e:\n      
       self.logger.error(f"Error parsing test cases: {str(e)}")\n             self.logger.debug(f"Raw LLM output:\\n{llm_output}")\n-        \n+            \n         return test_cases\n \n+    def _extract_field(self, text: str, pattern: str, default: str, flags: re.RegexFlag = 0) -> str:\n+        match = re.search(pattern, text, flags)\n+       
 return match.group(1).strip() if match else default\n+\n     def _save_results(self, pr: PullRequest, risk_analysis: dict, test_cases: List[Dict], output_file: str) -> None:\n         results = {\n             "pr_number": pr.number,\n             "pr_title": pr.title,\n             "risk_analysis": risk_analysis,\n             "test_cases": test_cases  # Already in dict format\n         }\n-        self.output_provider.write(results, output_file)\n\\ No newline at end of file\n+        self.output_provider.write(results, output_file)\n+\n+    def _display_risk_analysis(self, risk_analysis: Dict[str, Any]) -> None:\n+        table = Table(title="Risk Analysis Results")\n+        \n+        
table.add_column("Category", style="cyan")\n+        table.add_column("Details", style="magenta")\n+        \n+        # Safely get risk level\n+        level = risk_analysis.get(\'level\', \'Unknown\')\n+        table.add_row("Risk Level", f"[bold]{level}[/bold]")\n+        \n+        # Safely handle factors and details\n+        factors = risk_analysis.get(\'factors\', [])\n+        details = risk_analysis.get(\'details\', [])\n+        \n+        # If we have both factors and details, zip them\n+        if factors and details:\n+            for factor, detail in zip(factors, details):\n+                table.add_row(str(factor), str(detail))\n+        # Otherwise just show factors\n+     
   elif factors:\n+            for factor in factors:\n+                table.add_row(str(factor), "")\n+        \n+        self.console.print("\\n")\n+        self.console.print(table)\n+        self.console.print("\\n")\n\\ No newline at end of file\n```\n\nFile: src/main.py\n```diff\n@@ -1,29 +1,47 @@\n from core.factories import factory_manager\n from core.test_case_generator import TestCaseGenerator\n from utils.file_utils import load_config\n+import logging\n+import sys\n+import os\n import argparse\n \n def main():\n-    parser = argparse.ArgumentParser(\n-        description=\'Generate test cases from PRs\'\n+    logging.basicConfig(\n+        level=logging.DEBUG,\n+        format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n     )\n-    parser.add_argument(\'repo\')\n-    parser.add_argument(\'pr_number\', type=int)\n-    parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n-    parser.add_argument(\'--config\', default=\'config.yaml\')\n-    args = parser.parse_args()\n+    logger = logging.getLogger(__name__)\n \n-    config = load_config(args.config)\n-    factory_manager.configure(config)\n+    try:\n+        config_path = os.path.join(os.path.dirname(__file__), \'config.yaml\')\n+        config = load_config(config_path)\n+        \n+        factory_manager.configure(config)\n \n-    vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n-    llm = factory_manager.llm_factory.create("litellm", \n-                                             model=config["providers"]["llm"]["litellm"]["model"], \n-                                             temperature=config["providers"]["llm"]["litellm"]["temperature"])\n-    output = factory_manager.output_factory.create("yaml")\n+        parser = argparse.ArgumentParser(\n+            description=\'Generate test cases from PRs\'\n+        )\n+        parser.add_argument(\'repo\')\n+        parser.add_argument(\'pr_number\', type=int)\n+        parser.add_argument(\'--output\', default=\'pr_test_cases.yaml\')\n+        parser.add_argument(\'--config\', default=\'config.yaml\')\n+        args = parser.parse_args()\n \n-    generator = TestCaseGenerator(vcs, llm, output)\n-    generator.generate(args.repo, args.pr_number, args.output)\n+        config = load_config(args.config)\n+        factory_manager.configure(config)\n+\n+        vcs = factory_manager.vcs_factory.create("github", token=config["providers"]["vcs"]["github"]["token"])\n+        llm = factory_manager.llm_factory.create("litellm", \n+                                                 model=config["providers"]["llm"]["litellm"]["model"], \n+                                                 temperature=config["providers"]["llm"]["litellm"]["temperature"])\n+        
output = factory_manager.output_factory.create("yaml")\n+\n+        generator = TestCaseGenerator(vcs, llm, output)\n+        generator.generate(args.repo, args.pr_number, args.output)\n+    except Exception as e:\n+        logger.error(f"Failed to initialize: {str(e)}")\n+        sys.exit(1)\n \n if __name__ == "__main__":\n     main()\n\\ No newline at end of file\n```\n\nFile: src/models/test_case.py\n```diff\n@@ -1,5 +1,6 @@\n from dataclasses import dataclass\n-from typing import List\n+from typing import List, Optional\n+from datetime import datetime\n \n @dataclass\n class TestCase:\n@@ -8,4 +9,8 @@ class TestCase:\n     priority: str\n     description: str\n     steps: List[str]\n-  
  expected_result: str\n\\ No newline at end of file\n+    expected_result: str\n+    generated_at: datetime = datetime.now()\n+    approved: bool = False\n+    approved_by: Optional[str] = None\n+    risk_factors: List[str] = None\n\\ No newline at end of file\n```\n\nFile: src/pr_test_cases.yaml\n```diff\n@@ -1,53 +1,71 @@\n pr_number: 1\n pr_title: Bump starlette from 0.27.0 to 0.40.0 in /src\n risk_analysis:\n-  level: Medium\n+  level: High\n   factors:\n-  - []\n+  - Security Risk\n+  - Dependency Changes\n+  details:\n+  - Security-sensitive code changes detected\n+  - Package dependencies modified\n test_cases:\n - id: TC-001\n-  title: No title\n-  priority: Medium\n-  description: 
No description\n+  title: Verify the security of the updated Starlette dependency in the application\n+  priority: High\n+  description: Check if any newly introduced vulnerabilities or security risks are\n+    present in the updated version (0.40.0) of Starlette, which could potentially\n+    impact the overall security of the application.\n   steps:\n-  - Import starlette module in a test script\n-  - Write a test function which was working with the previous version and check if\n-    it works as expected with the new version\n+  - Install the updated version of Starlette (0.40.0) in a separate development environment\n+  - Run a comprehensive vulnerability scan on the isolated environment using tools\n+    like OWASP ZAP or Bandit\n+  - Check for any reported vulnerabilities, security issues, or warnings related to\n+    the updated Starlette package\n   - \'\'\n-  expected_result: The test function should pass without any errors or unexpected\n-    behaviors.\n+  expected_result: The security scan should not reveal any critical or high-severity\n+    vulnerabilities, ensuring that the application remains secure. If any issues are\n+    found, they must be addressed and fixed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.979267\'\n+  approved: false\n+  approved_by: null\n - id: TC-002\n-  title: No title\n+  title: Ensure compatibility of the updated Starlette dependency with other dependencies\n+    in the application\n   priority: Medium\n-  description: No description\n+  description: Validate that the updated version (0.40.0) of Starlette is compatible\n+    with the existing versions of other dependencies within the application, to prevent\n+    any potential conflicts or errors during runtime.\n   steps:\n-  - Install and run OWASP ZAP or bandit on the new starlette codebase\n-  - Analyze the results for any potential security vulnerabilities\n+  - Create a development environment containing all the dependencies listed in the\n+    requirements.txt file (excluding Starlette for now)\n+  - Install the updated version of Starlette (0.40.0) in this environment\n+  - Run the application to check if there are any errors, warnings, or unexpected\n+    behavior caused by the updated Starlette dependency\n   - \'\'\n-  expected_result: The test should not find any critical or high severity security\n-    vulnerabilities in the new version.\n+  expected_result: The application should 
run smoothly without any critical errors\n+    or warning messages related to the updated Starlette package, demonstrating compatibility\n+    with other dependencies. If any 
issues occur, they must be resolved before merging\n+    this update.\n+  generated_at: \'2025-01-28T14:27:14.979975\'\n+  approved: false\n+  approved_by: null\n - id: TC-003\n-  title: No title\n-  priority: Medium\n-  description: No description\n-  steps:\n-  - Create a custom exception class\n-  - Use the custom exception in a route handler and trigger it intentionally\n-  - Check if the exception is handled correctly by starlette and an appropriate error\n-    message is returned to the client\n-  - \'\'\n-  expected_result: Starlette should be able to handle the custom exception gracefully,\n-    returning an appropriate error message to the client.\n-- id: TC-004\n-  title: No title\n+  title: Test error handling of the updated Starlette dependency in the application\n   priority: Medium\n-  description: No description\n+  description: Verify that the updated version (0.40.0) of Starlette handles errors\n+    and exceptions gracefully, ensuring a good user experience even when unexpected\n+    situations occur.\n   steps:\n-  
- Create a test script which uses an API or functionality from starlette 0.27.0\n-  - Run the test script with both versions (0.27.0 and 0.40.0) of starlette installed\n-    separately\n-  - Check if there are any differences in behavior or outcomes between the two versions\n+  - Create test scenarios where exceptional conditions are intentionally 
triggered\n+    within the application (e.g., by providing invalid inputs or forcing certain edge\n+    cases)\n+  - Observe how the updated Starlette dependency handles these exceptions and errors,\n+    checking for appropriate logging, error messages, and recovery mechanisms\n   - \'\'\n-  expected_result: There should not be any noticeable differences in behavior or outcomes\n-    when using the same API or functionality with both versions (0.27.0 and 0.40.0)\n-    of starlette.\n+  expected_result: The updated Starlette dependency should handle errors effectively,\n+    providing descriptive error messages to help troubleshoot issues and maintaining\n+    application stability in the event of exceptional conditions. If any issues occur,\n+    they must be addressed before merging this update.\n+  generated_at: \'2025-01-28T14:27:14.980992\'\n+  approved: false\n+  approved_by: null\n```\n\nFile: src/prompts/pr_test_case_prompt.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/prompts/templates/test_case.txt\n```diff\n@@ -2,28 +2,29 @@ Given the following pull request changes:\n \n Title: {pr_title}\n Description: {pr_description}\n-Risk Level: {risk_level}\n+\n+Risk Assessment:\n+- Level: {risk_level}\n+- Factors: {risk_factors}\n \n Changed Files:\n {changes}\n \n Diffs:\n {diffs}\n \n-Generate test cases in this exact format (maintain the exact structure and indentation):\n+Generate specific test cases addressing the identified risk factors.\n+Focus on security, compatibility, and error handling.\n+\n+Format each test case exactly as follows:\n \n TC-001:\n-- Title: [Clear test objective]\n-- Priority: [High/Medium/Low]\n-- Description: [Detailed test description]\n+- Title: [Specific test objective]\n+- Priority: [Based on risk level]\n+- Description: [Detailed scenario]\n - Steps:\n-  - [Specific step 1]\n-  - [Specific step 2]\n-- Expected Results: [Clear expected outcome]\n-\n-Generate at least 3 test cases focusing on:\n-- Code changes shown in diffs\n-- Security implications\n-- Error handling\n-- Edge cases\n-- Backwards compatibility\n\\ No newline at end of file\n+  - [Clear, actionable step]\n+  - [Expected interaction]\n+- Expected Results: [Verifiable outcome]\n+\n+Generate at least 3 test cases.\n\\ No newline at end of file\n```\n\nFile: src/services/llm/llm_service.py\n```diff\n@@ -27,29 +27,21 @@ def generate(self, prompt: str, context: Dict[str, Any]) -> str:\n         return result\n \n     def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:\n-        pr = context.get(\'pr\', {})\n-        risk = context.get(\'risk_analysis\', {})\n-        \n-        # Format changes\n-        changes_str = "\\n".join(\n-            f"{k}: {\', \'.join(v)}" \n-            for k, v in pr.get(\'changes\', {}).items() \n-            if v\n-        )\n-        \n-        # Format diffs\n-        diffs_str = "\\n".join(\n-            f"File: {fname}\\n{diff}" \n-            for fname, diff in pr.get(\'diffs\', {}).items()\n-        )\n-        \n-        return prompt.format(\n-            pr_title=pr.get(\'title\', \'\'),\n-            pr_description=pr.get(\'description\', \'\'),\n-            risk_level=risk.get(\'level\', \'\'),\n-            changes=changes_str,\n-            diffs=diffs_str\n-        )\n+        try:\n+            return prompt.format(\n+                pr_title=context.get(\'pr_title\', \'\'),\n+                pr_description=context.get(\'pr_description\', \'\'),\n+                risk_level=context.get(\'risk_level\', \'\'),\n+                risk_factors=context.get(\'risk_factors\', \'\'),\n+                changes=context.get(\'changes\', \'\'),\n+     
           diffs=context.get(\'diffs\', \'\')\n+            )\n+        except KeyError as e:\n+            self.logger.error(f"Missing required key in context: {e}")\n+      
      raise\n+        except Exception as e:\n+            self.logger.error(f"Error formatting prompt: {e}")\n+            raise\n \n     def _format_changes(self, changes: Dict[str, Any]) -> str:\n         result = []\n```\n\nFile: src/utils/file_utils.py\n```diff\n@@ -1,5 +1,40 @@\n+import os\n import yaml\n+import logging\n+from typing import Dict, Any\n+import re\n \n-def load_config(path: str) -> dict:\n-    with open(path, \'r\', encoding=\'utf-8\') as f:\n-        return yaml.safe_load(f)\n\\ No newline at end of file\n+def load_config(path: str) -> Dict[str, Any]:\n+    logger = logging.getLogger(__name__)\n+    \n+    if not os.path.exists(path):\n+        logger.error(f"Config file not found: {path}")\n+        raise FileNotFoundError(f"Config file not found: {path}")\n+        \n+    with open(path, \'r\') as f:\n+        config = yaml.safe_load(f)\n+        \n+    if not config:\n+        raise ValueError("Empty config file")\n+        \n+    # Replace environment variables\n+    config = _replace_env_vars(config)\n+    \n+    return config\n+\n+def _replace_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:\n+    """Recursively replace ${VAR} with environment variable values"""\n+    if isinstance(config, dict):\n+        return {k: _replace_env_vars(v) for k, v in config.items()}\n+    elif isinstance(config, list):\n+        return [_replace_env_vars(v) for v in config]\n+    elif isinstance(config, str):\n+        pattern = r\'\\${([^}]+)}\'\n+        match = re.search(pattern, config)\n+        if match:\n+            env_var = match.group(1)\n+            env_value = os.getenv(env_var)\n+            if not env_value:\n+                raise ValueError(f"Environment variable {env_var} not set")\n+     
       return config.replace(match.group(0), env_value)\n+    return config\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analysis.py\n```diff\n@@ -1,24 +1,45 @@\n from enum import Enum\n+from typing import Dict, List, Any\n+import re\n \n class RiskLevel(Enum):\n     LOW = "Low"\n     MEDIUM = "Medium"\n     HIGH = "High"\n \n-def analyze_diff(diff_content):\n-    """Analyze diff content for risk patterns."""\n-    risk_patterns = {\n-        "sql": "Database changes",\n-        "await": "Async flow changes",\n-        "try": "Error handling changes",\n-        "import": "Dependency changes",\n-        "class": "Class structure changes",\n-        "function": "Function signature changes"\n-    }\n+class RiskFactor(Enum):\n+    DEPENDENCY = "Dependency Changes"\n+    SECURITY = "Security Impact"\n+    BREAKING = "Breaking Changes"\n+    COVERAGE = "Test Coverage"\n \n-    found_patterns = []\n-    for pattern, risk in risk_patterns.items():\n-        if pattern in diff_content.lower():\n-            found_patterns.append(risk)\n-\n-    return found_patterns\n\\ No newline at end of file\n+def analyze_risk(changes: Dict[str, List[str]], diffs: Dict[str, str]) -> Dict[str, Any]:\n+    risk_factors = []\n+    \n+    # Check dependency changes\n+    if any(\'requirements.txt\' in file or \'package.json\' in file for file in changes.get(\'modified\', [])):\n+        risk_factors.append(RiskFactor.DEPENDENCY.value)\n+    \n+    # Check security patterns\n+    security_patterns = [\n+        r\'auth[orization]*\',\n+        r\'password\',\n+  
      r\'secret\',\n+        r\'token\',\n+        r\'crypt\'\n+    ]\n+    \n+    if any(re.search(\'|\'.join(security_patterns), diff, re.I) for diff in diffs.values()):\n+ 
       risk_factors.append(RiskFactor.SECURITY.value)\n+    \n+    # Determine risk level\n+    risk_level = RiskLevel.LOW\n+    if len(risk_factors) >= 2:\n+        risk_level = RiskLevel.HIGH\n+    elif len(risk_factors) == 1:\n+        risk_level = RiskLevel.MEDIUM\n+    \n+    return {\n+        "level": risk_level.value,\n+        "factors": risk_factors\n+    }\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_analyzer.py\n```diff\n@@ -0,0 +1,85 @@\n+from typing import Dict, List, Any, Union\n+from enum import Enum\n+import re\n+import logging\n+\n+class RiskLevel(Enum):\n+    LOW = "Low"\n+    MEDIUM = "Medium"\n+    HIGH = "High"\n+\n+class RiskAnalyzer:\n+    def __init__(self):\n+        self.logger = logging.getLogger(__name__)\n+        self.security_patterns = [\n+            r\'auth\\w*\',\n+            r\'password\',\n+            r\'secret\',\n+            r\'token\',\n+            r\'crypt\'\n+        ]\n+        self.breaking_patterns = [\n+            r\'break.*change\',\n+            r\'deprecat\\w*\',\n+ 
           r\'remov\\w+\\s+\\w+\',\n+            r\'delet\\w+\\s+\\w+\'\n+        ]\n+\n+    def analyze(self, changes: Union[Dict[str, List[str]], str], diffs: Union[Dict[str, str], str]) -> Dict[str, Any]:\n+        try:\n+            # Normalize inputs\n+            changes_dict = changes if isinstance(changes, dict) else {"modified": [str(changes)]}\n+            diffs_dict = diffs if isinstance(diffs, dict) else {"file": str(diffs)}\n+            \n+            risk_factors = []\n+            details = []\n+\n+    
        # Check security risks\n+            if self._check_security_risks(diffs_dict):\n+                risk_factors.append("Security Risk")\n+                details.append("Security-sensitive code changes detected")\n+\n+            # Check dependency changes\n+            if self._check_dependency_changes(changes_dict):\n+                risk_factors.append("Dependency Changes")\n+                details.append("Package dependencies modified")\n+\n+            # Check breaking changes\n+            if self._check_breaking_changes(diffs_dict):\n+                risk_factors.append("Breaking Changes")\n+                details.append("Breaking changes detected")\n+\n+            level = self._determine_risk_level(risk_factors)\n+            \n+            return {\n+                "level": level,\n+                "factors": risk_factors,\n+                "details": details\n+            }\n+        except Exception as e:\n+            self.logger.error(f"Error in risk analysis: {e}")\n+            return {\n+                "level": "High",\n+                "factors": ["Analysis Error"],\n+                "details": [str(e)]\n+            }\n+\n+    def _check_security_risks(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.security_patterns)\n+            for content in diffs.values()\n+  
      )\n+\n+    def _check_dependency_changes(self, changes: Dict[str, List[str]]) -> bool:\n+        dependency_files = [\'requirements.txt\', \'package.json\', \'build.gradle\', \'pom.xml\']\n+        modified = changes.get(\'modified\', [])\n+        return any(dep in str(file) for dep in dependency_files for file in modified)\n+\n+    def _check_breaking_changes(self, diffs: Dict[str, str]) -> bool:\n+        return any(\n+            any(re.search(pattern, content, re.I) for pattern in self.breaking_patterns)\n+  
          for content in diffs.values()\n+        )\n+\n+    def _determine_risk_level(self, factors: List[str]) -> str:\n+        return "High" if len(factors) >= 2 else "Medium" if factors else "Low"\n\\ No newline at end of file\n```\n\nFile: src/utils/risk_patterns.py\n```diff\n@@ -0,0 +1,22 @@\n+from enum import Enum\n+from typing import List, Pattern\n+import re\n+\n+class RiskPatternType(Enum):\n+    SECURITY = "security"\n+    BREAKING = "breaking"\n+    PERFORMANCE = "performance"\n+    COMPLEXITY = "complexity"\n+\n+class RiskPattern:\n+    def __init__(self, pattern: str, risk_type: RiskPatternType, weight: int = 1):\n+        self.pattern = re.compile(pattern, re.IGNORECASE)\n+  
      self.type = risk_type\n+        self.weight = weight\n+\n+RISK_PATTERNS = [\n+    RiskPattern(r\'auth|login|password|secret|token\', RiskPatternType.SECURITY, 3),\n+    
RiskPattern(r\'break.*change|deprecat|remove[d]?\\s+\\w+\', RiskPatternType.BREAKING, 2),\n+    RiskPattern(r\'performance|optimize|slow|fast\', RiskPatternType.PERFORMANCE, 2),\n+    RiskPattern(r\'complex|complicated|confusing\', RiskPatternType.COMPLEXITY, 1)\n+]\n\\ No newline at end of file\n```\n\n\nGenerate specific test cases addressing the identified risk factors.\nFocus on security, compatibility, and error handling.\n\nFormat each test case exactly as follows:\n\nTC-001:\n- Title: [Specific test objective]\n- Priority: [Based on risk level]\n- Description: [Detailed scenario]\n- Steps:\n  - [Clear, actionable step]\n  - [Expected interaction]\n- Expected Results: [Verifiable outcome]\n\nGenerate at least 3 test cases.\n\n', 'options': {'temperature': 0.7}, 'stream': False}'


2025-01-28 14:50:17,135 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-01-28 14:50:17,136 - httpx - DEBUG - load_verify_locations cafile='C:\\Projects\\qitops\\src\\venv\\Lib\\site-packages\\certifi\\cacert.pem'
â ¼ Generating test cases...2025-01-28 14:50:17,369 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=600.0 socket_options=None
â ‹ Generating test cases...2025-01-28 14:50:19,413 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CBD6F082D0>
2025-01-28 14:50:19,413 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-28 14:50:19,414 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-28 14:50:19,414 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-28 14:50:19,415 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-28 14:50:19,415 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
â ¼ Generating test cases...2025-01-28 14:50:30,083 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 28 Jan 2025 12:50:30 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-01-28 14:50:30,084 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-01-28 14:50:30,084 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-28 14:50:30,084 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-28 14:50:30,085 - httpcore.http11 - DEBUG - response_closed.started
2025-01-28 14:50:30,085 - httpcore.http11 - DEBUG - response_closed.complete
14:50:30 - LiteLLM:INFO: utils.py:949 - Wrapper: Completed Call, calling success_handler
2025-01-28 14:50:30,093 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
14:50:30 - LiteLLM:DEBUG: litellm_logging.py:982 - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-01-28 14:50:30,094 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
14:50:30 - LiteLLM:DEBUG: cost_calculator.py:599 - completion_response response ms: None
2025-01-28 14:50:30,094 - LiteLLM - DEBUG - completion_response response ms: None
14:50:30 - LiteLLM:DEBUG: cost_calculator.py:599 - completion_response response ms: None
2025-01-28 14:50:30,095 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=6000 socket_options=None
2025-01-28 14:50:30,095 - LiteLLM - DEBUG - completion_response response ms: None
2025-01-28 14:50:30,096 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=6000 socket_options=None
â ‹ Generating test cases...2025-01-28 14:50:32,132 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CBD6EEEFD0>
2025-01-28 14:50:32,133 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,133 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-28 14:50:32,134 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,134 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-28 14:50:32,135 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,141 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 28 Jan 2025 12:50:32 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-01-28 14:50:32,141 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-01-28 14:50:32,142 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,142 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-28 14:50:32,142 - httpcore.http11 - DEBUG - response_closed.started
2025-01-28 14:50:32,143 - httpcore.http11 - DEBUG - response_closed.complete
14:50:32 - LiteLLM:DEBUG: cost_calculator.py:330 - Returned custom cost for model=ollama/mistral - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0      
2025-01-28 14:50:32,143 - LiteLLM - DEBUG - Returned custom cost for model=ollama/mistral - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
2025-01-28 14:50:32,144 - services.llm.llm_service - DEBUG - LLM Response:
 TC-001:
   - Title: Verify secure handling of sensitive data in the user login process
   - Priority: High
   - Description: Test user login with hardcoded sensitive data to ensure that proper encryption and hashing methods are being used.
   - Steps:
     1. Set up a test account with sensitive data (e.g., password 'testpassword')
     2. Submit the login form with the test account credentials
   - Expected Results: The application should correctly handle the sensitive data, preventing unauthorized access and ensuring that the data is encrypted or hashed before storage.

   TC-002:
   - Title: Validate compatibility with older package dependencies
   - Priority: Medium
   - Description: Test the application's behavior when using older versions of package dependencies known to have security vulnerabilities.
   - Steps:
     1. Update the 'package.json' or equivalent file to use an older version of a dependency with known vulnerabilities.
     2. Run the application and perform basic functionality tests.
   - Expected Results: The application should still function properly, and any known vulnerabilities within the updated package dependency should not be exploitable. Additionally, the application should provide clear warning messages regarding the use of outdated dependencies.

   TC-003:
   - Title: Test error handling when encountering unexpected security patterns in code
   - Priority: Medium
   - Description: Test the application's ability to handle errors caused by the presence of known security risks, such as hardcoded sensitive data or insecure functions.      
   - Steps:
     1. Introduce a security risk (e.g., hardcode sensitive data) into the source code.
     2. Run the application and perform basic functionality tests.
   - Expected Results: The application should gracefully handle the error, providing clear and helpful error messages that guide developers in resolving the issue and ensuring that the security vulnerability is remediated.
2025-01-28 14:50:32,146 - core.test_case_generator - DEBUG - Title match: Verify secure handling of sensitive data in the user login process
2025-01-28 14:50:32,146 - core.test_case_generator - DEBUG - Title match: Validate compatibility with older package dependencies
2025-01-28 14:50:32,146 - core.test_case_generator - DEBUG - Title match: Test error handling when encountering unexpected security patterns in code
2025-01-28 14:50:32,149 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CBD6EEF490>

âœ… Results saved to pr_test_cases.yaml
â ‹ Generating test cases...2025-01-28 14:50:32,150 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,151 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-28 14:50:32,152 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,152 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-28 14:50:32,152 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,159 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 28 Jan 2025 12:50:32 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-01-28 14:50:32,159 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-01-28 14:50:32,160 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,160 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-28 14:50:32,160 - httpcore.http11 - DEBUG - response_closed.started
2025-01-28 14:50:32,161 - httpcore.http11 - DEBUG - response_closed.complete
14:50:32 - LiteLLM:DEBUG: cost_calculator.py:330 - Returned custom cost for model=ollama/mistral - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0      
2025-01-28 14:50:32,161 - LiteLLM - DEBUG - Returned custom cost for model=ollama/mistral - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
2025-01-28 14:50:32,162 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,162 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-28 14:50:32,162 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,162 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-28 14:50:32,162 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-28 14:50:32,169 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 28 Jan 2025 12:50:32 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-01-28 14:50:32,170 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-01-28 14:50:32,170 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-28 14:50:32,171 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-28 14:50:32,171 - httpcore.http11 - DEBUG - response_closed.started
2025-01-28 14:50:32,171 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-28 14:50:32,248 - httpcore.connection - DEBUG - close.started
2025-01-28 14:50:32,248 - httpcore.connection - DEBUG - close.complete
2025-01-28 14:50:32,251 - httpcore.connection - DEBUG - close.started
2025-01-28 14:50:32,251 - httpcore.connection - DEBUG - close.complete
2025-01-28 14:50:32,251 - httpcore.connection - DEBUG - close.started
2025-01-28 14:50:32,252 - httpcore.connection - DEBUG - close.complete